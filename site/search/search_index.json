{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Rov's Tech Blog!","text":"<p>Welcome to my site! I'm Rov, a Microsoft Cloud DevOps Engineer working in Australia. I have a passion for technology and the ways it can be used in an enterprise environment to improve productivity and efficiency.</p> <p>If a task can be automated, then it should be!</p> <p>I have a strong background in Microsoft technologies. If it has been developed by Microsoft, then I have probably worked with it at some point in my career. I have extensive experience with Azure, Office 365, Windows Server, Active Directory, Exchange, SQL Server, SharePoint, PowerShell, and many other Microsoft technologies.</p> <p>I am one of those weirdos that do this not only as a job but also for fun. I am always looking for new technologies to learn and new ways to improve my skills.</p> <p>Anyway, enough about me\u2014I hope you enjoy my blogs and find them useful. If you have any questions or comments, please feel free to reach out to me! I am always happy to help where I can.</p>"},{"location":"#home-lab-projects","title":"\ud83c\udfe0 Home Lab Projects","text":""},{"location":"#kubernetes-setup-journey","title":"Kubernetes Setup Journey","text":"<ul> <li>Night 1 - Development Environment - Setting up WSL, Nix, and the development environment</li> <li>Night 2 - Hardware Setup - Configuring Dell OptiPlex hardware and system discovery</li> </ul>"},{"location":"#docker-swarm-with-dokploy","title":"Docker Swarm with Dokploy","text":"<ul> <li>Deployment Plan - Step-by-step guide to building a production-ready home lab</li> <li>My Home Lab Guide - Complete architecture and battle-tested solutions</li> </ul>"},{"location":"#security-identity-management","title":"\ud83d\udd10 Security &amp; Identity Management","text":""},{"location":"#windows-defender-application-control-wdac","title":"Windows Defender Application Control (WDAC)","text":"<ul> <li>Getting Started - Introduction to WDAC implementation</li> <li>Configuration - Advanced configuration techniques</li> </ul>"},{"location":"#conditional-access","title":"Conditional Access","text":"<ul> <li>Authentication - Setting up conditional access policies</li> <li>Deployment - Deployment strategies and best practices</li> </ul>"},{"location":"#privileged-identity-management-pim","title":"Privileged Identity Management (PIM)","text":"<ul> <li>Getting Started - PIM fundamentals and setup</li> <li>Groups Management - Managing privileged groups with PIM</li> </ul>"},{"location":"#development-devops","title":"\ud83d\ude80 Development &amp; DevOps","text":""},{"location":"#version-control","title":"Version Control","text":"<ul> <li>Introduction - Version control fundamentals</li> <li>AZ-400 Certification - Azure DevOps certification guide</li> <li>Azure Data Factory - Version control for data pipelines</li> <li>Data Ingestion Pipelines - Managing data pipeline versions</li> <li>Power BI - Version control for Power BI projects</li> <li>Power Pages - Managing Power Pages with version control</li> </ul>"},{"location":"#networking","title":"\ud83c\udf10 Networking","text":""},{"location":"#tailscale","title":"Tailscale","text":"<ul> <li>Homelab Configuration - Setting up Tailscale for secure home lab access</li> </ul>"},{"location":"#creative-writing","title":"\ud83c\udfae Creative Writing","text":""},{"location":"#game-lore","title":"Game Lore","text":"<ul> <li>Higher Power - Fantasy world building</li> <li>Of Elves - Elven lore and culture</li> <li>The Gnomes - Gnomish society and traditions</li> </ul> <p>Navigation Tips</p> <ul> <li>Use the navigation menu above to browse by category</li> <li>All posts are searchable using the search function</li> <li>Each post includes detailed explanations and practical examples</li> <li>Code snippets are copyable with the copy button</li> </ul> <p>About This Site</p> <p>This blog is built with MkDocs and the Material theme, hosted on GitHub Pages. All content is written in Markdown and version controlled with Git.</p>"},{"location":"Game/HigherPower/","title":"Chapter 1: Higher Power","text":"<p>The world of Althum\u2014a land shaped by magic, marred by war, and steeped in ancient mysteries\u2014has, after generations of conflict, settled into a fragile peace. This tenuous balance holds for now, but beneath the surface, old grudges simmer and empires hunger for more.</p> <p>At the center of this era stands the Kingdom of Bandor, the heart of humanity\u2019s power and ambition. Named after the legendary First Men, and still ruled by the ancient Bandor bloodline, the kingdom stretches proudly across the northern face of the world. Its territory spans from the storm-swept Darkstone Mountains to the windswept valleys of Torgoth Pass, positioning it as a vital trade nexus between the hardy Dwarves of Darkstone and the rest of the known world.</p> <p>Once a loose collection of fortified settlements, Bandor grew rapidly through iron will, strategic marriages, and military might. Its expansion reached as far north as the lush woodlands of Ember Shire, a modest village founded by the noble Eldora family. With Bandor\u2019s patronage, Ember Shire flourished, and the Eldoras grew powerful in their own right. Yet ambition breeds rebellion. Claiming the wild northern lands as their own, the Eldora family broke from Bandor and declared independence, founding the Kingdom of Eldora.</p> <p>The Bandor king, enraged by this betrayal, launched a campaign to reclaim the north. After a brutal war and the tragic razing of Ember Shire, the Eldoras bent the knee, swearing fealty once more. Though tensions remained, the Bandors permitted them to keep their northern crown\u2014so long as their loyalty was never questioned again.</p> <p>With the north secured, Bandor\u2019s gaze turned southwestward to the desolate Torgoth Highlands\u2014a barren expanse once home to the now-extinct orc clans. Though harsh and unforgiving, the Highlands held strategic value, and their conquest promised untold wealth from new trade routes. The Torgoth family, fierce warriors descended from barbarian stock, volunteered to lead the first expedition. They vanished into the wilderness and were not heard from again for many years.</p> <p>Subsequent expeditions met the same fate. At last, Bandor conscripted the Eldoras to send a force to uncover the Highlands\u2019 secrets. What they found was staggering.</p> <p>The Torgoth family had survived, even thrived. Hardened by the wilds, they had carved fortresses into the cliffs and transformed into something more brutal, more primal. The Eldora envoys made contact\u2014but diplomacy failed. When Bandor and Eldora armies marched to reclaim the Highlands, they were met by unyielding resistance. The Torgoth had fortified every pass and fought like the very stone itself. The campaign ended in a costly retreat. The Highlands, once thought conquered, remained firmly in Torgoth hands.</p> <p>Bandor, frustrated by this stinging defeat, pushed southward. They founded the village of Bandor\u2019s Edge, perched on the border of the mysterious Emerald Forest. It was a mistake. The ancient Elves of the Emerald stirred from their long slumber, displeased by the human encroachment. These elves\u2014bloodthirsty, nomadic, and in tune with the ancient spirits\u2014worshipped forgotten gods and offered sacrifices to sustain their power and unnaturally long lives.</p> <p>Some say the elves were the first to wield magic. Others claim the breakaway Silverleaf elves taught humanity the arcane arts out of pity. The noble houses, especially Bandor, insist that magic was a divine gift bestowed only upon worthy human bloodlines. No matter the truth, Bandor\u2019s Edge suffered. Despite walls and weapons, the village could not hold back the spectral raids and whispering shadows. Eventually, it was pulled back from the forest\u2019s edge, left half-abandoned and ever-watchful. The elves have not attacked in many years\u2014but they have not forgotten.</p> <p>Still seeking glory, the aging Bandor king launched one final expedition, pushing to the southern seas. Along the coast, he founded Everstill Bay, a lonely fishing port built beside strangely still waters. It now stands as a gateway to the unexplored world beyond\u2014the edge of Bandor\u2019s influence and the first stone in the foundation of something greater.</p>"},{"location":"Game/OfElves/","title":"Chapter 2: Of the Elves","text":"<p>The Elves of Althum are believed by many to be the first race\u2014ancient spirits of the forest, born from the world itself. They live in nomadic woodland tribes, deeply in tune with nature, magic, and the ever-whispering spirits of the land.</p> <p>But though they walk in harmony with nature, they are vicious, territorial, and ruthlessly reclusive. Few who trespass into their sacred woods ever return. To the elves, the forests are not just home\u2014they are holy. And they will bleed the earth red to protect them.</p> <p>\u2e3b</p>"},{"location":"Game/OfElves/#dark-rituals","title":"Dark Rituals","text":"<p>On the night of the fullest moon, the forest trembles with dark energy. Deep beneath the emerald canopy, elfkind gathers in a sacred crescent, kneeling before the Idol of Ael\u2019Nareth\u2014a towering effigy of a long-forgotten elven priestess, one arm stretched toward the heavens, the other pouring enchanted water from an open palm into a stone fountain.</p> <p>Their chants begin low, rhythmic, and haunting, growing louder as the moon reaches its zenith. Their eyes shimmer with eerie, ethereal blue light. From the trees emerges a high priestess, draped in silken robes that seem woven from moonlight and shadow. At her side, two armored guards drag forth a slumped human captive, his wrists bound, his face pale with terror.</p> <p>The crowd falls silent.</p> <p>The priestess murmurs sacred words as she approaches the fountain. Drawing a curved ceremonial dagger\u2014its edge glowing with old magic\u2014she gazes down at the trembling man. One guard jerks his head back, exposing the neck. With cruel precision, the priestess slices deep, blood spilling in thick rivers across the cold earth.</p> <p>As the human cries out, his voice echoing through the trees, the priestess begins to chant louder, her voice rising above the screaming wind. Blood lifts from the forest floor into the air, swirling into a crimson orb above them. Her teeth sharpen; her eyes turn black as pitch. With a feral screech, she sinks her teeth into the man\u2019s neck, tearing out his throat in a burst of gore.</p> <p>The crowd erupts in exultation.</p> <p>The floating orb of blood is guided by the priestess\u2019s hand, flowing into the mouth of the fountain. The waters run red. As more captives are dragged forward, the elves descend into a frenzy, feasting upon the flesh of the dead in a horrifying communion with their gods.</p> <p>\u2e3b</p>"},{"location":"Game/OfElves/#the-separated-tribes","title":"The Separated Tribes","text":"<p>Long ago, Althum was a world of endless forests. The elves thrived in unity, each tribe holding its own sacred groves and spiritual pacts. They lived in harmony\u2014not only with the wilds, but with one another.</p> <p>But then the humans arrived.</p> <p>At first, the elves responded with subtlety\u2014hit-and-run raids, cursed whispers in the night, illusions so haunting they shattered men\u2019s minds. But humanity was resilient. Settlements grew. Forests were cleared. Roads carved through sacred land.</p> <p>The war escalated. Cities rose, and with them came fire and steel. Many elven tribes were slaughtered, driven into exile, or left to rot beneath felled trees. Only the most savage endured\u2014and they embraced the darkness.</p> <p>Turning to forbidden magic, the surviving tribes delved into blood rites, using the essence of their enemies to fuel their power. Their magic grew cruel, violent, and feral. No longer protectors of nature\u2014they became its avengers, believing themselves the rightful rulers of the land, and humans as vermin to be culled.</p> <p>But with the forests dying, the elves were cut off from one another, forced into isolated fragments of their once-great domain.</p> <p>\u2e3b</p>"},{"location":"Game/OfElves/#the-traitor-elves","title":"The Traitor Elves","text":"<p>Yet not all elves embraced hatred.</p> <p>One large tribe, exiled from the deeper woods, began to see humanity differently. They watched how humans built, adapted, thrived\u2014and realized that the world was changing. Rather than fight it, they chose to evolve.</p> <p>This tribe established Silverleaf, a settlement that blended elven grace with human structure. They traded openly, forged friendships, and even began to teach humans the secrets of magic.</p> <p>To the other elves, this was unforgivable treachery.</p> <p>The \u201cSilverleaf traitors\u201d were exiled, hunted, and killed if they dared set foot in any forest claimed by the old tribes. But among humans, these elves were revered. Kings and queens welcomed them as tutors, advisors, and court mages. Their teachings laid the foundation for the magical arts of noble bloodlines.</p> <p>Thus began the great schism of elfkind\u2014between those who walk in blood, and those who walk in peace.</p>"},{"location":"Tailscale/Homelab%20Config/","title":"Tailscale","text":"<p>Current configuration through Unraid tailscale plugin</p> <p>Jellyfin is being funneled through Tailscale to allow access to the server from outside the home network.</p> <p>Commands to run on the server to get Tailscale working:</p> <pre><code>tailscale funnel --bg 8096\n\ntailscale status\n</code></pre> <p>will return something like this:</p> <pre><code># Funnel on:\n#     - https://tower.risk-lenok.ts.net\n\nhttps://tower.risk-lenok.ts.net (Funnel on)\n|-- / proxy http://127.0.0.1:8096\n</code></pre>"},{"location":"Tailscale/Homelab%20Config/#update","title":"Update!","text":"<p>This is now handled through the Unraid Tailscale plugin directly. Jellyfin itself is now funneled without exposing any other applications on Unraid. It also auto starts, and keeps its own persistant URL.</p>"},{"location":"Tailscale/Homelab%20Config/#update-2","title":"Update 2","text":"<p>I am no longer using Unraid. Tailscale fulleling for metal0 has been enabled to that Jellyfin gets exposed</p>"},{"location":"Version%20Control/AZ-400/","title":"AZ-400 - Microsoft Devops Engineer Expert","text":"<ul> <li>Design and implement processes and communications (10 - 15%)</li> <li>Design and implement a source control strategy (10 - 15%)</li> <li>Design and implement build and release pipelines (50 - 55%)</li> <li>Develop a security and compliance plan (10 - 15%)</li> <li>Implement an instrumentation strategy (5 - 10%)</li> </ul>"},{"location":"Version%20Control/AZ-400/#four-pillars-of-continuous-intergration","title":"Four pillars of continuous Intergration","text":"<p>Version Control system &gt; Package Management system &gt; Continuous Intergration system &gt; Automated build process</p> Continuous Integration Continuous Delivery Increase code coverage Automatically deploy code to production Build faster by splitting test and build runs Ensure deployment targets have the latest code Automatically ensure you dont ship broken code Use tested code from the CI process Run tests continually"},{"location":"Version%20Control/AZ-400/#key-terms-fro-azure-pipelines","title":"Key terms fro Azure Pipelines","text":"Term Description Agent An agent is installable software that runs a build deployment or deployment job. Artifact An Artifact is a collection of files or packages published by a build. Build A build represents the execution of a pipeline. Continuous delivery The process by which code is built, tested, and deployed to one or more test and production stages. Continuous Intergration The practice used by development teams to simplify the testing and building of code. Deployment target A deployment target is a VM, COntainer&lt; Web App, or any service used to host the developed application Job A build will contain one or more jobs. Most jobs run on an agent. Pipeline A pipeline defines the continuous intergration and deployment process for your app. It is made of steps called tasks. Release When you use the visual designer, you can create a release or build pipeline. A release is a term used to describe one exxecution of a release pipeline. Its made up of deployments to multiple stages. Stage Stages are the primary devisions in a pipeline. Task"},{"location":"Version%20Control/AZ-400/#github-tags","title":"GitHub Tags","text":""},{"location":"Version%20Control/AZ-400/#release-versions","title":"Release Versions","text":"<p>Tag specific commits or releases as versions. Follow the semantic versioning notation, broadly accepted as de facto standard for labeling software releases. It is common to prefix your version names with the letter v (e.g. v1.0.0 or v2.3.4). If the tag isn\u2019t meant for production use, add a pre-release version after the version name (e.g. v0.2.0-alpha or v5.9-beta.3). This helps users and collaborators easily distinguish between stable releases and features that are still in the testing phase.</p>"},{"location":"Version%20Control/AZ-400/#feature-releases","title":"Feature releases","text":"<p>Tag commits or merges that introduce new features or significant changes to the codebase. Use descriptive tags like \"feature/new-login-page\" or \"feature/payment-integration\" to highlight specific feature releases. These tags provide a clear history of feature additions and help track the progress of individual features over time.</p>"},{"location":"Version%20Control/AZ-400/#bug-fixes","title":"Bug Fixes","text":"<p>Tag commits or merges that address bug fixes or issues reported by users or collaborators. Use tags like bugfix/issue123\" or \"fix/critical-bug\" to denote bug fix releases. Bug fix tags make it easy to identify commits that resolve specific issues and ensure that fixes are applied consistently across different versions of the project.</p>"},{"location":"Version%20Control/AZ-400/#maintenance-releases","title":"Maintenance releases","text":"<p>Tag commits or merges that involve maintenance tasks such as code refactoring, documentation updates, or dependency upgrades. Use tags like \"maintenance/code-refactor\" or \"update/documentation\" to mark maintenance releases. Maintenance release tags help track changes related to code maintenance and ensure that necessary updates are documented and applied correctly.</p>"},{"location":"Version%20Control/AZ-400/#custom-tags","title":"Custom Tags","text":"<p>Create custom tags to categorize commits or releases based on your organization's specific needs or workflows. For example, you can create tags like \"documentation\", \"performance\", or \"security\" to classify commits according to their focus areas. Custom tags provide additional flexibility in organizing and structuring your codebase according to your project's requirements.</p>"},{"location":"Version%20Control/AzureDataFactory/","title":"Design Document: Client Data Ingestion &amp; Reporting Platform","text":""},{"location":"Version%20Control/AzureDataFactory/#objective","title":"Objective","text":"<p>To implement a scalable, secure, and automated data pipeline that ingests monthly data from client APIs, stores it in Microsoft Fabric OneLake, and generates customized Power BI reports through a controlled Dev/Test/Prod deployment process. Each client\u2019s data and reports must be securely separated, in accordance with ISO certification requirements.</p>"},{"location":"Version%20Control/AzureDataFactory/#1-system-overview","title":"1. System Overview","text":"<p>Core Technologies * Ingestion: Azure Data Factory (ADF) * Storage: Microsoft Fabric OneLake (Delta Lake format) * Processing: Fabric Lakehouse (with optional Dataflows Gen2) * Reporting: Power BI with Deployment Pipelines * Version Control: GitHub * Security: Azure RBAC, Row-Level Security (RLS), OneLake folder-level isolation</p>"},{"location":"Version%20Control/AzureDataFactory/#2-architecture-summary","title":"2. Architecture Summary","text":"<p>High-Level Flow 1.  Monthly Trigger in ADF initiates data collection for each client. 2.  Data Ingestion from external client APIs using parameterized pipelines. 3.  Data Storage in OneLake with client-specific folder paths. 4.  Power BI Workspaces per client access only their data. 5.  Power BI Deployment Pipelines manage Dev \u2192 Test \u2192 Prod progression. 6.  Reports are securely shared with individual clients.</p>"},{"location":"Version%20Control/AzureDataFactory/#3-data-ingestion-layer-azure-data-factory","title":"3. Data Ingestion Layer (Azure Data Factory)","text":"<p>Pipeline Design * Parameterization: Accepts client_id, api_url, token. * Scheduling: Monthly triggers per client. * Secrets: Stored in Azure Key Vault.</p> <p>Steps 1.  Connect to client API (REST connector) 2.  Fetch and parse data (JSON/CSV/XML) 3.  Write data to OneLake as Delta files:</p> <pre><code>/Clients/{client_id}/yyyy-MM/data.delta\n</code></pre>"},{"location":"Version%20Control/AzureDataFactory/#4-data-storage-layer-microsoft-fabric-onelake","title":"4. Data Storage Layer (Microsoft Fabric OneLake)","text":"<p>Storage Model * Shared Lakehouse for all clients * Client-specific folders to enforce isolation * Data stored in Delta format for optimal querying</p> <p>Example Structure</p> <pre><code>OneLake Workspace: Client-Data-Lakehouse\n/\n\u251c\u2500\u2500 Clients/\n\u2502   \u251c\u2500\u2500 ClientA/\n\u2502   \u2502   \u2514\u2500\u2500 2025-07/\n\u2502   \u2502       \u2514\u2500\u2500 data.delta\n\u2502   \u2514\u2500\u2500 ClientB/\n\u2502       \u2514\u2500\u2500 2025-07/\n\u2502           \u2514\u2500\u2500 data.delta\n</code></pre>"},{"location":"Version%20Control/AzureDataFactory/#5-reporting-layer-power-bi","title":"5. Reporting Layer (Power BI)","text":"<p>Workspace Strategy</p> <p>Each client receives their own Power BI workspaces:</p> <ul> <li>ClientA-Dev</li> <li>ClientA-Test</li> <li>ClientA-Prod</li> <li>ClientB-Dev</li> <li>ClientB-Test</li> <li>ClientB-Prod</li> </ul> <p>Deployment Pipeline * Dev: Development and initial connection to client data * Test: Internal QA and refresh validation * Prod: Final client-ready reports</p> <p>GitHub Integration * PBIX files stored and versioned in client-specific GitHub repos * Power BI REST API or Fabric GitHub Actions used for CI/CD</p>"},{"location":"Version%20Control/AzureDataFactory/#6-data-access-security","title":"6. Data Access &amp; Security","text":"<p>Access Control * OneLake Folder Isolation: Each Power BI dataset only connects to the client\u2019s folder. * RBAC: Azure AD roles restrict access to each workspace. * Optional RLS: If multi-client model is needed (not recommended here)</p> <p>Sharing * Power BI Apps: Used to bundle and share client reports * Power BI Embedded: Optional for embedding into client portals with token-based access</p> <p>Compliance * All pipelines and storage follow ISO-compliant best practices * Audit logs and access control are enforced via Azure</p>"},{"location":"Version%20Control/AzureDataFactory/#7-automation-monitoring","title":"7. Automation &amp; Monitoring","text":"<p>Automation * ADF pipeline triggers: Monthly per client * Dataset refreshes: Scheduled or event-triggered post-ingestion * GitHub Actions deploy ADF and Power BI artifacts from repo</p> <p>onitoring Tools * ADF Activity Monitoring * Power BI Refresh History * Azure Monitor for alerts and logs</p>"},{"location":"Version%20Control/AzureDataFactory/#8-naming-conventions","title":"8. Naming Conventions","text":"<p>Workspaces</p> <p>{ClientName}-{Stage} Examples: - AcmeCorp-Dev - AcmeCorp-Test - AcmeCorp-Prod</p> <p>OneLake Paths <pre><code>/Clients/{client_id}/{yyyy-MM}/data.delta\n</code></pre></p>"},{"location":"Version%20Control/AzureDataFactory/#9-demo-setup-guide-full-walkthrough","title":"9. Demo Setup Guide (Full Walkthrough)","text":"<p>Step 1: Provision Infrastructure * Create Resource Group * Create OneLake Lakehouse workspace * Create Key Vault and set API secrets</p> <p>Step 2: Create and Publish GitHub Repo * Use a repo template with ADF pipeline JSON, ARM/Bicep deployment scripts * Push to GitHub and configure Actions</p> <p>Step 3: Deploy Azure Data Factory * Use ARM/Bicep to deploy ADF instance * Deploy ingestion pipeline using GitHub Actions on push to main or publish branch</p> <p>Step 4: Configure ADF Pipeline * Parameterize pipeline for client_id, api_url, and auth_key * Set monthly trigger * Store secrets in Key Vault, reference them in Linked Services</p> <p>Step 5: Setup OneLake Folder * Create /Clients/DemoClient/yyyy-MM/ folder * Confirm pipeline writes data to Delta file format</p> <p>Step 6: Create Power BI Workspaces * Create Dev, Test, Prod workspaces using Power BI REST API or manually * Register them in Fabric deployment pipeline UI</p> <p>Step 7: Version Control Power BI Reports * Save reports as PBIP format in GitHub * On PR or merge to main, deploy report via GitHub Action or manual trigger</p> <p>Step 8: Connect Power BI Dataset to OneLake * Use delta/Parquet connector to point to the demo folder * Load and model dataset</p> <p>Step 9: Secure Access * Use Azure AD security groups to restrict workspace access * Create Power BI App and share with demo users</p>"},{"location":"Version%20Control/AzureDataFactory/#10-future-enhancements","title":"10. Future Enhancements","text":"<ul> <li>Implement Azure Purview for data catalog and classification</li> <li>Automate workspace and pipeline creation using PowerShell + REST APIs</li> <li>Add retry logic and alerting to ADF pipelines</li> <li>Expand GitHub workflows to support multiple branches and environments</li> </ul>"},{"location":"Version%20Control/AzureDataFactory/#11-conclusion","title":"11. Conclusion","text":"<p>This design ensures scalable, secure, and maintainable data processing across multiple clients. It enables customized reporting through isolated workspaces and centralized storage, while enforcing strict data segregation and compliance with ISO standards. The added step-by-step demo guide supports rapid prototyping and testing of the architecture in your own environment.</p>"},{"location":"Version%20Control/IngestionPipelines/","title":"Data Ingestion Pipelines Version Control","text":"<p>This section covers version control strategies for data ingestion pipelines, including best practices for managing pipeline configurations, dependencies, and deployment processes.</p> <p>Coming Soon</p> <p>This content is currently being developed. Check back soon for comprehensive guidance on version controlling data ingestion pipelines.</p>"},{"location":"Version%20Control/Intro/","title":"Introduction to Git Version Control (Specifically for PowerPlatform and 365)","text":""},{"location":"Version%20Control/Intro/#application-lifecycle-management-alm","title":"Application Lifecycle Management (ALM)","text":""},{"location":"WDAC/GettingStarted/","title":"Windows Defender Application Control (WDAC)","text":""},{"location":"WDAC/GettingStarted/#introduction","title":"Introduction","text":"<p>Windows Defender Application Control (WDAC) is a powerful security feature in Windows that helps organizations control which applications and scripts can run on their endpoints. Deploying WDAC via Microsoft Intune ensures policy enforcement across managed devices, providing a robust defense against... Anyone doing litteraly anything outside of what you allow them to do.</p>"},{"location":"WDAC/GettingStarted/#understanding-wdac","title":"Understanding WDAC","text":"<p>WDAC allows organizations to define rules that permit only approved applications and scripts to run. Unlike traditional application control methods, WDAC leverages code integrity policies and can operate in audit or enforced mode. </p> <p>Obviously Audit mode is just that, it audits. It doesn't block anything, but reports on anything that may have been blocked. Enforced mode is the opposite, it blocks anything that hasn't been approved.</p> <p>DANGER</p> <p>WDAC is a powerful security feature that can have a significant impact on your environment. Before deploying WDAC, it is important to understand how it works and how it will affect your organization. ALWAYS run it on a small subset of devices first to ensure it doesn't break anything, even after you run in audit mode and think you have it all under control. Ask me how I know...</p>"},{"location":"WDAC/GettingStarted/#getting-started","title":"Getting Started","text":"<p>OK so to get started we should firt go over everyrhing you are going to need to get started.</p> <ul> <li>Defender for Endpoint P2 license</li> <li>Windows 10 Enterprise or Education</li> <li>Intune enrolled devices</li> <li>WDAC Wizard, or even better, App Control Manager. (Please go and check out SpyNetGirl's App Control Manager)</li> <li>Device Groups based on your requirements</li> </ul> <p>So basically what you will be doing is creating an audit mode policy to block everything bar Micorsoft signed. You will then monitor this in the Security centre for a few weeks to see what is getting blocked. Cross check this with your list of approved apps. Then build a supplemental policy to allow these apps to run. So in a list:</p> <ul> <li>Create a list of apporved apps</li> <li>Create a base policy to block all bar MS signed</li> <li>Monitor for a few weeks</li> <li>Create a supplemental policy to allow approved apps that are blocked by the base policy</li> <li>Monitor for a few weeks</li> <li>Deploy the policy to a small subset of devices in enforced</li> <li>update as required</li> </ul> <p>NOTE</p> <p>Lucky for you Intune adds a few benifits that mean you dont have to keep updating the supplemental policy. We will get to all that soon though.</p>"},{"location":"WDAC/GettingStarted/#next-time","title":"Next time","text":"<p>Check out my next article on configuration of WDAC. We will go over the steps to configure a base policy and supplemental policy using SpyNetGirl's App Control Manager.</p>"},{"location":"WDAC/configuration/","title":"Drop\u2011in first post","text":""},{"location":"WDAC/configuration/#introduction","title":"Introduction","text":"<p>Welcome to my first post on Windows Defender Application Control (WDAC), also known as App Control for Business. This post will walk you through creating your first WDAC policy using SpyNetGirl's AppControl Manager tool, which simplifies the process significantly.</p>"},{"location":"WDAC/configuration/#wdac-capabilities-at-a-glance","title":"WDAC capabilities at a glance","text":"<p>WDAC is a powerful security feature in Windows that allows you to control which applications and scripts can run on your devices. Key capabilities include:</p> <ul> <li>Creating base and supplemental policies</li> <li>Defining rules based on file attributes like publisher, file hash, path, and more</li> <li>Running in audit mode to monitor without blocking</li> <li>Enforcing policies to block unapproved applications</li> <li>Generating detailed event logs for analysis</li> </ul>"},{"location":"WDAC/configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Windows 10 or Windows 11 device with administrative privileges</li> <li>PowerShell 5.1 or later</li> <li>A test app you expect to be blocked (so we can generate audit events).</li> </ul>"},{"location":"WDAC/configuration/#install-appcontrol-manager","title":"Install AppControl Manager","text":"<p>Option A \u2014 Microsoft Store (recommended): - Open Microsoft Store and install AppControl Manager.</p> <p>Option B \u2014 One\u2011liner (offline\u2011friendly bootstrap from GitHub): <pre><code>(irm 'https://raw.githubusercontent.com/HotCakeX/Harden-Windows-Security/main/Harden-Windows-Security.ps1')+'AppControl'|iex\n</code></pre></p>"},{"location":"WDAC/configuration/#create-a-base-policy","title":"Create a base policy","text":"<p>Open AppControl Manager &gt; Create AppControl Policy &gt; Choose a starting template: - DefaultWindows_Audit - allows Windows + drivers + Store apps. - Allow Microsoft - adds trust for Microsoft-signed apps (Office, Teams, etc.)</p> <p>Set these policy rule option sin the UI: - Enabled: Audit Mode - Enabled: Allow Supplemental Policies - Managed Installer (If you use Intune or ConfigMgr to deploy your apps.)</p> <p>Using Audit mode first allows you to deploy your policy to a group of test users and collect real world audit telemetry that you can use to creaft your supplemental policies later.</p>"},{"location":"WDAC/configuration/#deploy","title":"Deploy","text":"<p>Deploy the base policy to your devices via Group Policy, Intune, or local deployment:</p> <ul> <li>For local deployment, run:</li> </ul> <pre><code>Set-CIPolicy -PolicyPath \"C:\\Path\\To\\YourPolicy.xml\" -Deploy\n</code></pre> <ul> <li>For Intune, upload the XML as a custom OMA-URI policy.</li> </ul>"},{"location":"WDAC/configuration/#generate-audit-data","title":"Generate audit data","text":"<p>To gather audit data for supplemental policies, switch your base policy to audit mode:</p> <pre><code>Set-CIPolicy -PolicyPath \"C:\\Path\\To\\YourPolicy.xml\" -Audit\n</code></pre> <p>Deploy this audit mode policy to your devices and collect event logs over time.</p>"},{"location":"WDAC/configuration/#create-supplemental-policy","title":"Create supplemental policy","text":"<p>Use AppControl Manager to create supplemental policies based on audit events:</p> <ol> <li>Review audit events to identify blocked applications you want to allow.</li> <li>Create a supplemental policy in AppControl Manager.</li> <li>Add rules for the identified applications.</li> <li>Link the supplemental policy to your base policy.</li> <li>Export and deploy the supplemental policy.</li> </ol>"},{"location":"WDAC/configuration/#flip-to-enforce","title":"Flip to enforce","text":"<p>After testing and verifying your policies, switch from audit to enforce mode to block unapproved applications:</p> <pre><code>Set-CIPolicy -PolicyPath \"C:\\Path\\To\\YourPolicy.xml\" -Enforce\n</code></pre> <p>Deploy the enforce mode policy to your devices.</p>"},{"location":"WDAC/configuration/#whats-next","title":"What\u2019s next","text":"<p>In upcoming posts, I will cover:</p> <ul> <li>Deep dive into WDAC capabilities</li> <li>Using AppControl Manager advanced features</li> <li>Building supplemental policies from audit logs</li> <li>Deploying policies with Intune</li> <li>Managing multiple policies and updates</li> <li>WDAC hardening and ongoing management strategies</li> </ul> <p>Stay tuned!</p>"},{"location":"WDAC/configuration/#wdac-app-control-for-business-capabilities","title":"WDAC (App Control for Business) Capabilities","text":""},{"location":"WDAC/configuration/#overview","title":"Overview","text":"<p>This post will explore WDAC/App Control for Business capabilities, including: - Policy rule options - File rule levels - Event logging - Audit vs Enforce modes</p> <p>Coming soon...</p>"},{"location":"WDAC/configuration/#building-supplemental-policies","title":"Building Supplemental Policies","text":""},{"location":"WDAC/configuration/#overview_1","title":"Overview","text":"<p>This post will show how to build supplemental policies from audit events, link them to a base policy, and deploy them.</p> <p>Coming soon...</p>"},{"location":"WDAC/configuration/#wdac-deployment-with-intune","title":"WDAC Deployment with Intune","text":""},{"location":"WDAC/configuration/#overview_2","title":"Overview","text":"<p>This post will detail deploying WDAC policies via Microsoft Intune, including both built-in App Control for Business and custom XML/OMA-URI deployments.</p> <p>Coming soon...</p>"},{"location":"WDAC/configuration/#managing-multiple-wdac-policies","title":"Managing Multiple WDAC Policies","text":""},{"location":"WDAC/configuration/#overview_3","title":"Overview","text":"<p>This post will explain base vs supplemental, side-by-side policies, and changes in recent Windows updates.</p> <p>Coming soon...</p>"},{"location":"WDAC/configuration/#wdac-hardening-and-ongoing-management","title":"WDAC Hardening and Ongoing Management","text":""},{"location":"WDAC/configuration/#overview_4","title":"Overview","text":"<p>This post will cover using Microsoft's recommended driver and app block lists, handling packaged apps, and maintaining WDAC policies over time.</p> <p>Coming soon...</p>"},{"location":"homelab/Dokploy/MyHomeLab/","title":"My Home Lab, Rebuilt: A Practical, Battle-Tested Guide","text":"<p>I recently tore down and rebuilt my home lab on Docker Swarm. This post walks through the architecture, the why behind key choices, and the gritty fixes that made everything fast, reliable, and maintainable. If you\u2019re wrangling media, smart-home gear, and photos\u2014all while keeping stateful data safe on NAS\u2014this is for you.</p> <p>What You'll Learn</p> <p>This guide covers a production-ready home lab setup using Docker Swarm with real-world solutions for common problems like NFS storage, network discovery, and database performance issues.</p>"},{"location":"homelab/Dokploy/MyHomeLab/#topology-at-a-glance","title":"Topology at a Glance","text":"<ul> <li>Cluster: Docker Swarm across metal0 \u2026 metal6 (7-node cluster for high availability)</li> <li>Orchestrator UX: Dokploy to deploy stacks from a GitHub repo (compose-as-code approach)</li> <li>Ingress: Traefik on the overlay network <code>dokploy-network</code> routing public hostnames</li> <li>Tunnels: Cloudflare Tunnel for select services (e.g., Home Assistant front door)</li> <li>Storage Architecture:<ul> <li>Rockstor at 192.168.1.184 for persistent appdata (NFS v4.1)</li> <li>Synology NAS#2 at 192.168.1.154 (\u201cPlex Media\u201d share, NFS v3) for media &amp; Immich uploads</li> <li>Synology NAS#1 at 192.168.1.153 (\u201cemby_media\u201d share, NFS v3) for legacy media</li> </ul> </li> <li>Networking Strategy (split-brain by design):<ul> <li>Host networking where discovery/UPnP matters (Jellyfin, Home Assistant)</li> <li>Overlay networking for everything else, with an edge sidecar pattern to bridge to host-net apps</li> </ul> </li> </ul> <p>Why Docker Swarm?</p> <p>Docker Swarm provides native clustering with built-in load balancing, service discovery, and rolling updates. Unlike Kubernetes, it's lightweight and perfect for home labs where simplicity matters more than enterprise features.</p>"},{"location":"homelab/Dokploy/MyHomeLab/#architecture-overview","title":"Architecture Overview","text":""},{"location":"homelab/Dokploy/MyHomeLab/#why-this-setup-works","title":"Why This Setup Works","text":"<p>This home lab architecture is designed around three core principles:</p> <ol> <li>Separation of Concerns: Storage, compute, and networking are handled by specialised components</li> <li>Hybrid Networking: Use the right network mode for each application's requirements</li> <li>Data Locality: Keep performance-critical data local while sharing configuration via NFS</li> </ol>"},{"location":"homelab/Dokploy/MyHomeLab/#hardware-foundation","title":"Hardware Foundation","text":"<p>The cluster consists of seven nodes (metal0-metal6) running Docker Swarm: - metal0: Primary node with fastest storage (SSD) for databases - metal1-metal6: Worker nodes with varying capabilities - Network: Gigabit Ethernet with plans for 10GbE upgrade - Storage: Mix of local SSDs for databases and NFS for shared data</p>"},{"location":"homelab/Dokploy/MyHomeLab/#core-design-patterns-that-pay-off","title":"Core Design Patterns That Pay Off","text":""},{"location":"homelab/Dokploy/MyHomeLab/#1-nfs-done-right-match-the-server-and-the-path","title":"1) NFS Done Right: Match the Server and the Path","text":"<p>Network File System (NFS) is the backbone of this setup, providing shared storage across the cluster. The key insight is that different NAS systems require different NFS versions and mounting approaches.</p> <p>I use NFS v4.1 for Rockstor appdata and NFS v3 for Synology shares. On Synology, the export includes spaces (<code>:/volume1/Plex Media</code>), so you must pass the device exactly as exported and quote correctly in Compose.</p> <p>NFS Version Compatibility</p> <p>Mixing NFS versions incorrectly will cause mount failures. Always check your NAS export settings and match the version exactly. Synology typically uses NFS v3, while modern Linux NAS solutions like Rockstor prefer v4.1.</p> <p>Example volume definitions:</p> <pre><code># Rockstor appdata (NFS v4.1)\ndriver_opts:\n  type: nfs\n  o: addr=192.168.1.184,nfsvers=4.1,rw\n  device: :/export/appdata/&lt;app&gt;/config\n\n# Synology shares (NFS v3)\ndriver_opts:\n  type: nfs\n  o: addr=192.168.1.154,nfsvers=3,rw\n  device: \":/volume1/Plex Media\"\n</code></pre>"},{"location":"homelab/Dokploy/MyHomeLab/#quick-sanity-checks-i-use","title":"Quick Sanity Checks I Use","text":"<p>Before deploying any application that relies on NFS, always test the mount manually:</p> <pre><code># Create a probe volume\ndocker volume create \\\n  --driver local \\\n  --opt type=nfs \\\n  --opt o=addr=192.168.1.154,nfsvers=3,rw \\\n  --opt device=\":/volume1/Plex Media\" \\\n  plex_media_probe\n\n# Write a marker to prove the mount\ndocker run --rm -v plex_media_probe:/t alpine sh -lc \\\n 'mkdir -p \"/t/images\"; echo ok &gt; \"/t/images/.probe\"; ls -lah \"/t/images\"'\n</code></pre> <p>NFS Troubleshooting</p> <p>If the probe fails, check these common issues:</p> <ul> <li>Export path: Verify the exact path on your NAS (including spaces)</li> <li>NFS version: Match your NAS server's supported version</li> <li>Permissions: Ensure the NFS export allows your Docker host's IP</li> <li>Firewall: Check that NFS ports (2049, 111) are open</li> </ul> <p>If this fails, fix the export or the nfsvers before deploying the app.</p> <p>Edge Sidecar Pattern Benefits</p> <p>This architectural pattern provides several advantages:</p> <ul> <li>Network isolation: Keep most services on secure overlay networks</li> <li>Service discovery: Host networking apps can still use multicast/broadcast</li> <li>Clean routing: External access through Traefik with proper SSL termination</li> <li>Flexibility: Easy to add authentication middleware or rate limiting</li> </ul>"},{"location":"homelab/Dokploy/MyHomeLab/#2-edge-sidecar-to-bridge-host-net-apps-to-traefik","title":"2) \u201cEdge sidecar\u201d to bridge host-net apps to Traefik","text":"<p>Some apps (Home Assistant, Jellyfin) benefit from network_mode: host for multicast/discovery or DLNA. But I still want pretty hostnames via Traefik. The trick is a tiny nginx sidecar on the overlay that proxies to the app\u2019s LAN IP:port. - App (host net) binds to the node (e.g., metal4:8123). - ha-edge (overlay) exposes / \u2192 proxies to 192.168.1.194:8123. - Traefik routes assistant.cooked.beer to the sidecar. This keeps all the \u201csmart home\u201d discovery local while giving me a clean public entry (via Cloudflare Tunnel or Traefik).</p>"},{"location":"homelab/Dokploy/MyHomeLab/#3-keep-hot-lock-sensitive-dbs-local-the-jellyfin-fix","title":"3) Keep Hot, Lock-Sensitive DBs Local (The Jellyfin Fix)","text":"<p>SQLite databases and NFS don't mix well\u2014you'll eventually encounter \"database is locked\" errors and poor performance. This is because SQLite relies on file locking mechanisms that don't work reliably over network filesystems.</p> <p>SQLite + NFS = Problems</p> <p>Never run SQLite databases directly on NFS shares. The file locking mechanisms SQLite relies on don't work properly over network filesystems, leading to corruption and performance issues.</p>"},{"location":"homelab/Dokploy/MyHomeLab/#storage-architecture-deep-dive","title":"Storage Architecture Deep Dive","text":""},{"location":"homelab/Dokploy/MyHomeLab/#understanding-nfs-in-a-home-lab-context","title":"Understanding NFS in a Home Lab Context","text":"<p>Network File System (NFS) is crucial for sharing configuration and data across cluster nodes, but it requires careful planning:</p> <p>NFS Version Selection: - NFS v3: Stateless protocol, better for simple file sharing (Synology default) - NFS v4.1: Stateful with better security and performance features (modern Linux NAS)</p> <p>Performance Considerations: - Network bandwidth: Ensure adequate network capacity for concurrent access - Latency sensitivity: Keep databases local, share configuration via NFS - Caching: Enable client-side caching where appropriate</p> <p>Mount Options Explained: <pre><code># Common NFS mount options\nrw          # Read-write access\nsoft        # Fail gracefully on timeout (use with caution)\nhard        # Retry indefinitely (safer for critical data)\nintr        # Allow interruption of NFS calls\ntimeo=600   # Timeout in deciseconds (60 seconds)\nretrans=2   # Number of retries before giving up\n</code></pre></p> <p>NFS Performance Impact</p> <p>While NFS is convenient for shared configuration, it introduces network latency. For performance-critical applications like databases, always use local storage with NFS only for configuration and logs.</p> <p>The solution is to overlay just the database path onto a local volume while keeping the rest of the configuration on NFS: <pre><code>volumes:\n  - jellyfin_config_v2:/config          # NFS (settings/plugins/logs)\n  - jellyfin_cache_v2:/cache            # NFS (artwork cache)\n  - jellyfin_db_local:/config/data      # LOCAL (DB files only)\n  - type: tmpfs                         # Transcode scratch = RAM\n    target: /transcode\n    tmpfs:\n      size: 8g\n</code></pre> I also pinned Jellyfin to metal0 (which has the fastest disk) via Swarm constraint/label.</p> <p>Result: Fast library scans, no more database locks, and reliable performance.</p> <p>Database Performance Strategy</p> <p>For any application with a local database (SQLite, LevelDB, etc.):</p> <ol> <li>Keep databases local - Use node-local storage for DB files</li> <li>Pin to specific nodes - Use Swarm constraints to ensure consistency</li> <li>Separate concerns - Keep config/cache on NFS, databases local</li> <li>Use fast storage - SSDs make a huge difference for database performance</li> </ol>"},{"location":"homelab/Dokploy/MyHomeLab/#the-applications","title":"The Applications","text":""},{"location":"homelab/Dokploy/MyHomeLab/#jellyfin-media-server","title":"Jellyfin (Media Server)","text":"<p>Jellyfin is the heart of the media setup, serving movies, TV shows, and music to devices throughout the home.</p> <p>Why host networking? DLNA/UPnP discovery protocols work best with direct network access. These protocols rely on multicast and broadcast packets that don't traverse Docker's overlay networks well.</p> <p>Storage Strategy: - Config/Cache: Rockstor NFS v4.1 for settings, plugins, and artwork cache - Database: Local SSD storage at <code>/var/lib/jellyfin-db</code> for performance - Media Sources:     - <code>:/volume1/Plex Media</code> \u2192 mounted at <code>/media/plex</code> (NFS v3)     - <code>:/volume1/emby_media</code> \u2192 mounted at <code>/media/emby</code> (NFS v3)</p> <p>Performance Optimisations: - Transcoding: 8GB tmpfs at <code>/transcode</code> for fast temporary file operations - Node pinning: Constrained to metal0 (fastest storage) for consistent performance</p> <p>Access Methods: - Internal: Direct access on <code>http://192.168.1.190:8096</code> - External: Tailscale Funnel (e.g., <code>https://metal0.\u2026ts.net</code>) - No Traefik: Direct exposure avoids overlay network complexity for DLNA</p> <p>Why Not Traefik for Jellyfin?</p> <p>While Traefik works great for web apps, media servers benefit from direct network access for device discovery and DLNA streaming. The edge sidecar pattern adds unnecessary complexity here.</p>"},{"location":"homelab/Dokploy/MyHomeLab/#radarr-sonarr-prowlarr-sabnzbd-jellyseerr","title":"Radarr, Sonarr, Prowlarr, SABnzbd, Jellyseerr","text":"<ul> <li>Pattern: Straight overlay services behind Traefik.</li> <li>Downloads / media paths: Keep consistent with Jellyfin to avoid post-processing shenanigans.</li> <li>NFS: Synology shares use nfsvers=3, Rockstor appdata uses 4.1.</li> <li>Common gotcha: Mount errors like \u201cinvalid argument\u201d or \u201cno such file or directory\u201d almost always mean the export path or NFS version doesn\u2019t match reality. Example Traefik labels: <pre><code>deploy:\n  labels:\n    - traefik.enable=true\n    - traefik.http.routers.radarr.rule=Host(`radarr.cooked.beer`)\n    - traefik.http.routers.radarr.entrypoints=web\n    - traefik.http.services.radarr.loadbalancer.server.port=7878\n    - traefik.docker.network=dokploy-network\n    - traefik.http.routers.radarr.middlewares=authelia@docker\n</code></pre></li> </ul> <p>*arr Stack Configuration Tips</p> <ul> <li>Consistent paths: Use identical mount points across all *arr services and Jellyfin</li> <li>Authentication: Implement Authelia or similar for secure external access</li> <li>Resource limits: Set appropriate CPU and memory limits to prevent resource starvation</li> <li>Health checks: Configure proper health checks for reliable service management</li> </ul>"},{"location":"homelab/Dokploy/MyHomeLab/#vaultwarden-password-manager","title":"Vaultwarden (Password Manager)","text":"<p>Vaultwarden is a lightweight, self-hosted Bitwarden server implementation that provides enterprise-grade password management for the home lab.</p> <p>Architecture: Simple overlay service behind Traefik with authentication middleware.</p> <p>Storage: Configuration persisted to Rockstor (<code>/export/appdata/vaultwarden</code>) for backup and persistence across node failures.</p> <p>Security Best Practices</p> <ul> <li>Never commit secrets: Use Dokploy's environment variables or secrets management</li> <li>Enable 2FA: Configure TOTP for admin access</li> <li>Regular backups: Password databases are critical\u2014ensure they're included in your backup strategy</li> <li>Admin token rotation: Regularly rotate admin tokens and API keys</li> </ul>"},{"location":"homelab/Dokploy/MyHomeLab/#immich-self-hosted-photos","title":"Immich (self-hosted photos)","text":"<ul> <li> <p>Services: immich-server, immich-machine-learning, redis/valkey, and postgres.</p> </li> <li> <p>Where \u201cuploads\u201d live: Synology NAS#2 under :/volume1/Plex Media/images (NFS v3) mounted at /usr/src/app/upload. Immich expects a structure with thumbs, library, encoded-video, backups, etc. I verified folder health by dropping a marker file .immich inside critical dirs (especially encoded-video).</p> </li> <li> <p>DB: Immich\u2019s Postgres pinned to metal2 (label immichdb=1) with its data on the node\u2019s local disk (/srv/appdata/immich/postgres). The Immich team warns against network shares for the DB\u2014respect that.</p> </li> </ul> <p>Immich Performance Optimisation</p> <p>Machine Learning Performance:</p> <ul> <li>GPU acceleration: Consider adding GPU support for faster facial recognition</li> <li>Model caching: Store ML models on fast storage for quicker startup</li> <li>Memory allocation: Ensure adequate RAM for the ML container (4GB minimum)</li> </ul> <p>Storage Performance:</p> <ul> <li>Thumbnail generation: Use local SSD storage for thumbnail cache</li> <li>Video transcoding: Ensure adequate CPU or hardware acceleration</li> <li>Database tuning: Optimise PostgreSQL settings for photo metadata workloads</li> </ul> <ul> <li> <p>Model cache: Rockstor NFS (/export/appdata/immich/model-cache) for machine-learning models.</p> </li> <li> <p>Networking gotcha: immich-server must see redis in the same network. If you see getaddrinfo ENOTFOUND redis, put them on the same overlay or set REDIS_HOST env to the service DNS.</p> </li> </ul> <p>Immich Storage Health</p> <p>Immich requires specific directory structures for proper operation. Always verify that required directories exist and are writable:</p> <pre><code># Verify Immich directory structure\ndocker run --rm -v immich_uploads:/data alpine sh -c \"\nmkdir -p /data/{thumbs,library,encoded-video,backups,profile}\ntouch /data/encoded-video/.immich\nls -la /data/\n\"\n</code></pre> <p>If you see: <pre><code>### Home Assistant Deep Dive\n\nHome Assistant is the central hub for smart home automation, requiring careful network configuration to work properly with IoT devices.\n\n**Why Host Networking is Essential:**\n- **mDNS Discovery**: Many smart devices use multicast DNS for discovery\n- **SSDP Protocol**: UPnP devices rely on Simple Service Discovery Protocol\n- **Direct Device Communication**: Some integrations require direct IP communication\n- **Performance**: Eliminates network translation overhead for real-time automation\n\n**Network Requirements:**\n```bash\n# Essential ports for Home Assistant\nPort 8123/tcp  # Web interface\nPort 5353/udp  # mDNS (multicast DNS)\nPort 1900/udp  # SSDP (UPnP discovery)\nPort 21063/tcp # HomeKit (if enabled)\n</code></pre></p> <p>Integration Examples: - Philips Hue: Requires mDNS for bridge discovery - Sonos: Uses SSDP for speaker discovery - Chromecast: Relies on multicast for device detection - MQTT: Direct TCP communication with IoT devices Failed to read (/data/encoded-video/.immich): ENOENT ... <pre><code>create the directory and a .immich marker so Immich's health checks pass.\n\n### Home Assistant\nGoal: Discovery on the LAN and a pretty hostname with Cloudflare/Traefik.\n- HA runs host-net on metal4, binding to 8123 locally for SSDP/mDNS.\n- Edge sidecar (ha-edge) is an nginx container on the overlay that proxies to 192.168.1.194:8123.\n- Traefik routes assistant.cooked.beer \u2192 ha-edge.\n- Firewall (UFW on the node):\n    - 8123/tcp from LAN, 5353/udp (mDNS), 1900/udp (SSDP).\nMinimal, robust HA config (/config/configuration.yaml):\n\n```yaml\nhomeassistant:\n  name: Cooked Home\n  time_zone: Australia/Adelaide\n  unit_system: metric\n  currency: AUD\n\n# Essential services/UI\ndefault_config:\n\n# Reverse proxy hygiene\nhttp:\n  use_x_forwarded_for: true\n  trusted_proxies:\n    - 127.0.0.1\n    - 192.168.1.0/24\n    - 10.0.0.0/8\n    - 172.16.0.0/12\n</code></pre></p> <p>Home Assistant Configuration Gotchas</p> <ul> <li><code>external_url</code> / <code>internal_url</code> are optional; only use them if you have specific requirements</li> <li>Misplacing them or adding unsupported <code>http:</code> keys will break the UI completely</li> <li>Always test configuration changes in a development environment first</li> </ul> <p>The working sidecar (key bits): <pre><code>ha-edge:\n  image: nginx:alpine\n  networks: [dokploy-network]\n  environment:\n    - HA_UPSTREAM=192.168.1.194:8123\n  volumes:\n    - ha-nginx-template:/etc/nginx/templates:ro\n  deploy:\n    labels:\n      - traefik.enable=true\n      - traefik.docker.network=dokploy-network\n      - traefik.http.routers.ha.rule=Host(`assistant.cooked.beer`)\n      - traefik.http.routers.ha.entrypoints=web\n      - traefik.http.services.ha.loadbalancer.server.port=8080\n</code></pre> I generate the nginx config from a tiny init container so the environment variable substitution is bullet-proof (no shell quoting surprises).</p> <p>Nginx Template Pattern</p> <p>Using nginx templates with environment variable substitution is more reliable than shell-based config generation. The nginx:alpine image supports this natively via the <code>/etc/nginx/templates</code> directory.</p>"},{"location":"homelab/Dokploy/MyHomeLab/#backups-with-duplicacy-cli-swarm-cron","title":"Backups with Duplicacy CLI + Swarm Cron","text":"<p>I didn\u2019t want to pay for Duplicacy Web, so I wired Duplicacy CLI to run on a schedule using swarm-cronjob. - Target: Backblaze B2 bucket (S3-compatible endpoint).</p> <p>Backup Strategy Deep Dive</p> <p>Why Duplicacy?</p> <ul> <li>Deduplication: Saves significant storage space with incremental backups</li> <li>Encryption: Client-side encryption ensures data privacy</li> <li>Multiple backends: Supports various cloud storage providers</li> <li>Reliability: Lock-free design prevents corruption from interrupted backups</li> </ul> <p>Alternative backup solutions to consider:</p> <ul> <li>Restic: Similar features with different CLI interface</li> <li>Borg: Excellent for local backups with deduplication</li> <li>Rclone: Great for simple sync operations to cloud storage</li> </ul> <ul> <li>Repo: The Immich uploads volume (read-only).</li> <li>Schedule: Every 3 days at midnight.</li> <li>Retention: Keep the latest 2 snapshots.</li> <li>State: Duplicacy .duplicacy preferences live on Rockstor so jobs can run stateless anywhere. Key idea: A one-time init service (scaled to 0) and two cron-driven services (backup &amp; prune). The cron magic is all labels; no external scheduler needed.</li> </ul> <p>Example backup label: <pre><code>- swarm.cronjob.command=duplicacy -log -pref-dir /config/.duplicacy -repository /data backup -stats -threads 4\n</code></pre> Prune: <pre><code>- swarm.cronjob.command=duplicacy -log -pref-dir /config/.duplicacy -repository /data prune -a -keep 0:2\n</code></pre> B2 env (store as secrets, not plaintext): <pre><code>B2_ACCOUNT_ID=...\nB2_ACCOUNT_KEY=...\nDUPLICACY_PASSWORD=...                # if you choose to encrypt\nB2_BUCKET=cooked-photos\nB2_PREFIX=immich\n</code></pre></p>"},{"location":"homelab/Dokploy/MyHomeLab/#network-security-deep-dive","title":"Network Security Deep Dive","text":"<p>UFW Configuration Example: <pre><code># Basic UFW setup on each node\nsudo ufw default deny incoming\nsudo ufw default allow outgoing\n\n# SSH access (change port as needed)\nsudo ufw allow from 192.168.1.0/24 to any port 22\n\n# Docker Swarm cluster communication\nsudo ufw allow from 192.168.1.190/32 to any port 2377  # Manager\nsudo ufw allow from 192.168.1.0/24 to any port 7946   # Node communication\nsudo ufw allow from 192.168.1.0/24 to any port 4789   # Overlay network\n\n# Home Assistant specific\nsudo ufw allow from 192.168.1.0/24 to any port 8123   # Web interface\nsudo ufw allow from 192.168.1.0/24 to any port 5353   # mDNS\nsudo ufw allow from 192.168.1.0/24 to any port 1900   # SSDP\n\nsudo ufw enable\n</code></pre></p> <p>Firewall Safety</p> <p>Always test firewall rules carefully. Consider keeping a console session open when applying new rules in case you lock yourself out via SSH.</p>"},{"location":"homelab/Dokploy/MyHomeLab/#authentication-strategy","title":"Authentication Strategy","text":"<p>Authelia Integration: For services that don't have built-in authentication, Authelia provides: - Single sign-on across all services - Two-factor authentication support - Fine-grained access control - Integration with external identity providers</p> <p>Example Authelia middleware configuration: <pre><code># In your Traefik-enabled service\ndeploy:\n  labels:\n    - traefik.http.routers.service.middlewares=authelia@docker\n</code></pre></p>"},{"location":"homelab/Dokploy/MyHomeLab/#dokploy-swarm-the-workflow","title":"Dokploy + Swarm: The Workflow","text":"<p>Dokploy provides a GitOps-style deployment workflow for Docker Swarm, though it's still in beta and has room for improvement. Despite its current limitations, the core workflow is solid and production-ready.</p> <p>Dokploy Beta Status</p> <p>Dokploy is currently in beta, so expect some rough edges. However, the core GitOps workflow is stable enough for production home lab use. The development team is actively improving the platform.</p> <p>Deployment Process: - GitOps-ish approach: Each application has a <code>compose.yaml</code> in a Git repository - Automated deployment: Dokploy pulls changes and runs <code>docker stack deploy</code> - Service naming: Stacks appear as <code>apps-&lt;name&gt;-&lt;suffix&gt;_&lt;service&gt;</code> in Docker</p> <p>Essential Debugging Commands: <pre><code># Check service status and placement\ndocker service ps &lt;service&gt; --no-trunc\n\n# View service logs\ndocker service logs &lt;service&gt;\n\n# Inspect service configuration\ndocker service inspect &lt;service&gt; --format '{{.Spec.TaskTemplate.ContainerSpec.Mounts}}'\n\n# Test volume mounts\ndocker run --rm -v &lt;volume&gt;:/t alpine ls -lah /t\n\n# Get detailed error information\ndocker service ps &lt;service&gt; --no-trunc --format \"table {{.ID}}\\t{{.Name}}\\t{{.Image}}\\t{{.Node}}\\t{{.DesiredState}}\\t{{.CurrentState}}\\t{{.Error}}\"\n</code></pre></p> <p>Debugging Pro Tips</p> <ul> <li>Always use <code>--no-trunc</code> on error messages\u2014they contain the exact details about what went wrong</li> <li>Test volume mounts independently before deploying full stacks</li> <li>Check service placement with <code>docker service ps</code> to ensure proper node constraints</li> </ul>"},{"location":"homelab/Dokploy/MyHomeLab/#security-and-hygiene","title":"Security and Hygiene","text":"<ul> <li>Firewall on nodes: Open only what you need (HA\u2019s 8123, mDNS/SSDP from LAN).</li> <li>Secrets: Keep admin tokens, DB creds, and B2 keys out of git. Dokploy env/secrets work well.</li> <li>TLS strategy: Internal HTTP is fine for LAN; external paths go through Traefik + Tunnel with TLS terminated at the edge.</li> </ul>"},{"location":"homelab/Dokploy/MyHomeLab/#lessons-learned-so-you-dont-have-to","title":"Lessons Learned (so you don\u2019t have to)","text":"<ol> <li>NFS versions matter. Synology\u2019s NFS v3 exports behave differently from NFS v4/v4.1. Match the server.</li> <li>Spaces in export paths are real paths. Quote the device string exactly as exported.</li> <li>SQLite over NFS is fragile. Put the hot .db files on local ext4 and leave the bulk on NFS.</li> <li>Discovery wants host-net. Don\u2019t fight Home Assistant or DLNA. Bridge to Traefik with a sidecar instead.</li> <li>Cron in Swarm is easy. swarm-cronjob + containerized CLIs (Duplicacy) is clean and portable.</li> <li>Read errors literally. Most \u201cmount failed\u201d messages tell you the problem (wrong vers, wrong device, or permission).</li> </ol>"},{"location":"homelab/Dokploy/MyHomeLab/#advanced-topics-and-future-improvements","title":"Advanced Topics and Future Improvements","text":""},{"location":"homelab/Dokploy/MyHomeLab/#monitoring-and-observability","title":"Monitoring and Observability","text":"<p>While not covered in detail here, a production home lab benefits from proper monitoring:</p> <p>Monitoring Stack Recommendations</p> <ul> <li>Prometheus + Grafana: For metrics collection and visualisation</li> <li>Loki: For centralised log aggregation</li> <li>Uptime Kuma: For simple service availability monitoring</li> <li>Node Exporter: For hardware metrics from each cluster node</li> </ul>"},{"location":"homelab/Dokploy/MyHomeLab/#disaster-recovery-planning","title":"Disaster Recovery Planning","text":"<p>Backup Strategy Expansion: - Configuration backups: Regular exports of Dokploy configurations - Database dumps: Automated PostgreSQL and other database backups - Infrastructure as Code: Document all manual configurations for reproducibility</p>"},{"location":"homelab/Dokploy/MyHomeLab/#performance-tuning-and-optimisation","title":"Performance Tuning and Optimisation","text":""},{"location":"homelab/Dokploy/MyHomeLab/#docker-swarm-optimisation","title":"Docker Swarm Optimisation","text":"<p>Node Resource Management: <pre><code># Check node resource usage\ndocker node ls\ndocker system df\ndocker system events\n\n# Monitor service resource consumption\ndocker stats $(docker ps --format \"table {{.Names}}\" | grep -v NAMES)\n</code></pre></p> <p>Service Placement Strategies: - Database services: Pin to nodes with fastest storage (SSDs) - CPU-intensive services: Distribute across nodes with adequate CPU - Memory-heavy services: Consider memory constraints when placing services</p>"},{"location":"homelab/Dokploy/MyHomeLab/#network-performance","title":"Network Performance","text":"<p>Overlay Network Tuning: <pre><code># Check overlay network health\ndocker network ls\ndocker network inspect dokploy-network\n\n# Monitor network traffic\nsudo iftop -i docker_gwbridge\n</code></pre></p> <p>Bandwidth Considerations: - Media streaming: Ensure adequate bandwidth for multiple concurrent streams - Backup operations: Schedule during off-peak hours to avoid congestion - NFS traffic: Monitor NFS performance during peak usage</p> <p>Performance Monitoring</p> <p>Implement monitoring early to identify bottlenecks before they impact user experience. Tools like Prometheus, Grafana, and Node Exporter provide excellent visibility into system performance.</p>"},{"location":"homelab/Dokploy/MyHomeLab/#maintenance-and-updates","title":"Maintenance and Updates","text":""},{"location":"homelab/Dokploy/MyHomeLab/#regular-maintenance-tasks","title":"Regular Maintenance Tasks","text":"<p>Weekly: - Check service health and logs - Verify backup completion - Monitor storage usage</p> <p>Monthly: - Update container images - Review security logs - Test disaster recovery procedures</p> <p>Quarterly: - Update host operating systems - Review and rotate secrets - Capacity planning assessment</p> <p>Update Strategy</p> <p>Always test updates in a development environment first. Use Docker Swarm's rolling update capabilities to minimise downtime during production updates.</p>"},{"location":"homelab/Dokploy/MyHomeLab/#storage-performance-optimisation","title":"Storage Performance Optimisation","text":"<p>Tiered Storage Strategy: - Tier 1 (SSD): Databases, application caches, frequently accessed data - Tier 2 (NFS/HDD): Configuration files, logs, infrequently accessed media - Tier 3 (Cloud): Backups, archival data</p> <p>NFS Performance Tuning: <pre><code># Optimal NFS mount options for performance\nmount -t nfs -o rsize=1048576,wsize=1048576,hard,intr,timeo=600 \\\n  192.168.1.184:/export/appdata /mnt/appdata\n</code></pre></p>"},{"location":"homelab/Dokploy/MyHomeLab/#network-performance-optimisation","title":"Network Performance Optimisation","text":"<p>Bandwidth Planning: - 4K streaming: 25-50 Mbps per stream - Photo uploads: Burst bandwidth for mobile device sync - Backup operations: Schedule during off-peak hours</p> <p>Quality of Service (QoS): - Prioritise real-time traffic (Home Assistant, media streaming) - Limit backup bandwidth during peak hours - Monitor network utilisation patterns</p> <p>Scaling Considerations</p> <p>As your home lab grows, consider:</p> <ul> <li>Resource allocation: Monitor CPU and memory usage across nodes</li> <li>Storage growth: Plan for data growth, especially with photo/video services</li> <li>Network bandwidth: Ensure adequate bandwidth for backup and sync operations</li> <li>Power consumption: Balance performance with energy efficiency</li> </ul>"},{"location":"homelab/Dokploy/MyHomeLab/#conclusion","title":"Conclusion","text":"<p>This home lab setup provides a robust foundation for self-hosted services while maintaining simplicity and reliability. The key principles\u2014proper storage separation, network pattern consistency, and security hygiene\u2014apply regardless of your specific hardware or service choices.</p> <p>The combination of Docker Swarm's simplicity with Dokploy's GitOps workflow creates a maintainable platform that can evolve with your needs. While there are rough edges (especially with Dokploy being in beta), the core architecture is solid and production-ready for home use.</p> <p>Next Steps: - Implement comprehensive monitoring - Expand backup coverage to all critical services - Document disaster recovery procedures - Consider adding development/staging environments for testing changes</p>"},{"location":"homelab/Dokploy/MyHomeLab/#troubleshooting-quick-reference","title":"Troubleshooting Quick Reference","text":""},{"location":"homelab/Dokploy/MyHomeLab/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>NFS Mount Failures: <pre><code># Test NFS connectivity\nshowmount -e 192.168.1.154\n\n# Manual mount test\nsudo mount -t nfs -o nfsvers=3 192.168.1.154:/volume1/Plex\\ Media /mnt/test\n</code></pre></p> <p>Docker Swarm Service Issues: <pre><code># Check service status across all nodes\ndocker service ps --no-trunc &lt;service_name&gt;\n\n# View service configuration\ndocker service inspect &lt;service_name&gt;\n\n# Check node availability\ndocker node ls\n</code></pre></p> <p>Traefik Routing Problems: <pre><code># Check Traefik dashboard\ncurl -H \"Host: traefik.cooked.beer\" http://localhost:8080/dashboard/\n\n# Verify service discovery\ndocker service ls --filter label=traefik.enable=true\n</code></pre></p> <p>Getting Help</p> <p>When asking for help in forums or Discord:</p> <ul> <li>Include the exact error message (use <code>--no-trunc</code>)</li> <li>Share your Docker Compose configuration (sanitised)</li> <li>Mention your NFS server type and version</li> <li>Include relevant log snippets from <code>docker service logs</code></li> </ul>"},{"location":"homelab/Dokploy/MyHomeLab/#resources-and-references","title":"Resources and References","text":"<ul> <li>Dokploy Documentation: Official documentation for the deployment platform</li> <li>Docker Swarm Documentation: Comprehensive guide to Docker Swarm</li> <li>Traefik Documentation: Reverse proxy and load balancer configuration</li> <li>Duplicacy Documentation: Backup software configuration and best practices</li> <li>Home Assistant Documentation: Smart home automation platform</li> </ul> <p>Final Thoughts</p> <p>Building a reliable home lab is an iterative process. Start simple, document everything, and gradually add complexity as you understand each component. The patterns described here have been battle-tested in a production home environment and should serve as a solid foundation for your own setup.</p> <p>Key Success Factors:</p> <ul> <li>Start small: Begin with essential services and expand gradually</li> <li>Document everything: Your future self will thank you for detailed notes</li> <li>Test thoroughly: Always verify changes in a safe environment first</li> <li>Monitor actively: Implement observability from day one</li> <li>Plan for failure: Design with redundancy and recovery in mind</li> </ul>"},{"location":"homelab/Dokploy/Plan/","title":"The Plan: Building a Production-Ready Home Lab","text":"<p>This document outlines the step-by-step process for building a robust Docker Swarm cluster using Dokploy for orchestration. The plan covers everything from initial hardware setup to final cluster configuration.</p> <p>Prerequisites</p> <p>Before starting, ensure you have:</p> <ul> <li>3-7 mini PCs or servers for the cluster</li> <li>USB drives for OS installation</li> <li>Network access and static IP addresses planned</li> <li>Basic Linux administration knowledge</li> </ul>"},{"location":"homelab/Dokploy/Plan/#1-install-and-configure-mini-pcs","title":"1. Install and Configure Mini PCs","text":""},{"location":"homelab/Dokploy/Plan/#operating-system-installation","title":"Operating System Installation","text":"<ul> <li>Download and flash Ubuntu Server to USB drive (fix spelling: Ubuntu, not \"Unbuntu\") Installation Configuration:</li> <li>Minimal install: Reduces attack surface and resource usage</li> <li>OpenSSH enabled: Essential for remote management</li> <li>Static IP addresses: Required for stable cluster communication</li> </ul> <p>Installation Best Practices</p> <ul> <li>Use LVM for flexible storage management</li> <li>Enable automatic security updates during installation</li> <li>Set strong passwords or configure SSH key authentication</li> <li>Document MAC addresses for DHCP reservations if needed</li> </ul>"},{"location":"homelab/Dokploy/Plan/#node-configuration","title":"Node Configuration","text":"<p>Cluster Topology: - metal0 <code>192.168.1.190</code> - Primary manager node (done) - metal1 <code>192.168.1.191</code> - Worker node (done) - metal2 <code>192.168.1.192</code> - Worker node (done) - metal3 <code>192.168.1.193</code> - Worker node (done) - metal4 <code>192.168.1.194</code> - Worker node (done) - metal5 <code>192.168.1.195</code> - Secondary manager node (done) - metal6 <code>192.168.1.196</code> - Worker node (done)</p> <p>Manager Node Strategy</p> <p>Docker Swarm requires an odd number of manager nodes for quorum. With 7 nodes total, having 2 managers provides redundancy while maintaining cluster stability.</p>"},{"location":"homelab/Dokploy/Plan/#post-installation-security-hardening","title":"Post-Installation Security Hardening","text":"<p>Essential Security Setup: <pre><code># Update system packages\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install security tools\nsudo apt install fail2ban ufw htop curl wget git -y\n\n# Configure firewall\nsudo ufw allow ssh\nsudo ufw allow from 192.168.1.0/24  # Allow local network\nsudo ufw enable\n\n# Configure fail2ban for SSH protection\nsudo systemctl enable fail2ban\nsudo systemctl start fail2ban\n</code></pre></p> <p>Security Considerations</p> <ul> <li>Change default SSH port: Consider moving SSH to a non-standard port</li> <li>Disable root login: Ensure root SSH access is disabled</li> <li>Key-based authentication: Disable password authentication once SSH keys are configured</li> <li>Regular updates: Enable automatic security updates</li> </ul> <p>Additional Hardening Steps: <pre><code># Disable root login (edit /etc/ssh/sshd_config)\nsudo sed -i 's/#PermitRootLogin yes/PermitRootLogin no/' /etc/ssh/sshd_config\n\n# Restart SSH service\nsudo systemctl restart ssh\n\n# Set up automatic updates\nsudo apt install unattended-upgrades -y\nsudo dpkg-reconfigure -plow unattended-upgrades\n</code></pre></p>"},{"location":"homelab/Dokploy/Plan/#2-set-up-tailscale-vpn","title":"2. Set Up Tailscale VPN","text":"<p>Tailscale provides secure, encrypted connectivity between nodes and enables remote access to the cluster.</p>"},{"location":"homelab/Dokploy/Plan/#installation-process","title":"Installation Process","text":"<p>Install Tailscale on each node: <pre><code># Download and install Tailscale\ncurl -fsSL https://tailscale.com/install.sh | sh\n\n# Connect to your Tailscale network with SSH enabled\nsudo tailscale up --ssh --accept-routes\n</code></pre></p> <p>Why Tailscale?</p> <ul> <li>Zero-config VPN: Automatic mesh networking between devices</li> <li>SSH replacement: Built-in SSH access through the VPN</li> <li>MagicDNS: Automatic hostname resolution</li> <li>ACL support: Fine-grained access control policies</li> </ul>"},{"location":"homelab/Dokploy/Plan/#administrative-configuration","title":"Administrative Configuration","text":"<p>From Tailscale Admin Console: 1. Approve all devices - Authorise each node to join the network 2. Enable MagicDNS - Allows hostname-based connectivity 3. Rename nodes to match physical hostnames:    - metal0    - metal1    - metal2    - metal3    - metal4    - metal5    - metal6 4. Configure ACLs (optional) - Restrict access between nodes if needed</p>"},{"location":"homelab/Dokploy/Plan/#connectivity-testing","title":"Connectivity Testing","text":"<p>Verify network connectivity: <pre><code># Test basic connectivity\nping metal0.tailnet-name.ts.net\n\n# Test SSH connectivity\nssh metal1.tailnet-name.ts.net\n\n# Verify all nodes can reach each other\nfor i in {0..6}; do\n    echo \"Testing metal$i...\"\n    ping -c 1 metal$i.tailnet-name.ts.net\ndone\n</code></pre></p> <p>Tailscale Best Practices</p> <ul> <li>Use SSH keys: Configure SSH key authentication for passwordless access</li> <li>Enable Tailscale SSH: Simplifies access management and provides audit logs</li> <li>Monitor connections: Regularly check the admin console for unauthorised devices</li> </ul>"},{"location":"homelab/Dokploy/Plan/#3-install-docker-engine","title":"3. Install Docker Engine","text":"<p>Docker Engine is the foundation of the container orchestration platform. Install it on all nodes before setting up the Swarm cluster.</p>"},{"location":"homelab/Dokploy/Plan/#installation-process_1","title":"Installation Process","text":"<p>Install Docker on each node: <pre><code># Download and install Docker using the official script\ncurl -fsSL https://get.docker.com | sh\n\n# Add current user to docker group (avoid using sudo for docker commands)\nsudo usermod -aG docker $USER\n\n# Apply group membership (or logout/login)\nnewgrp docker\n\n# Enable Docker service to start on boot\nsudo systemctl enable docker\nsudo systemctl start docker\n</code></pre></p>"},{"location":"homelab/Dokploy/Plan/#post-installation-configuration","title":"Post-Installation Configuration","text":"<p>Verify Docker installation: <pre><code># Check Docker version\ndocker --version\n\n# Test Docker functionality\ndocker run hello-world\n\n# Check Docker service status\nsudo systemctl status docker\n</code></pre></p> <p>Configure Docker daemon (optional but recommended): <pre><code># Create Docker daemon configuration\nsudo mkdir -p /etc/docker\n\n# Configure logging and storage driver\nsudo tee /etc/docker/daemon.json &gt; /dev/null &lt;&lt;EOF\n{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  },\n  \"storage-driver\": \"overlay2\"\n}\nEOF\n\n# Restart Docker to apply configuration\nsudo systemctl restart docker\n</code></pre></p> <p>Docker Security</p> <ul> <li>Group membership: Adding users to the docker group grants root-equivalent access</li> <li>Log rotation: Configure log rotation to prevent disk space issues</li> <li>Resource limits: Consider setting memory and CPU limits for containers</li> <li>Registry security: Use trusted container registries and scan images for vulnerabilities</li> </ul>"},{"location":"homelab/Dokploy/Plan/#4-install-dokploy-on-the-manager-node","title":"4. Install Dokploy on the Manager Node","text":"<p>Dokploy is a modern deployment platform that provides a GitOps workflow for Docker Swarm. It includes all necessary components for a complete orchestration solution.</p>"},{"location":"homelab/Dokploy/Plan/#installation-process_2","title":"Installation Process","text":"<p>Install Dokploy on the primary manager node (metal0): <pre><code># Download and run the Dokploy installation script\ncurl -sSL https://dokploy.com/install.sh | sh\n</code></pre></p> <p>What Dokploy Installs</p> <p>The installation script automatically sets up:</p> <ul> <li>Dokploy: Web-based deployment platform</li> <li>Traefik: Reverse proxy and load balancer</li> <li>PostgreSQL: Database for Dokploy metadata</li> <li>Redis: Caching and session storage</li> <li>Docker Swarm: Initialises the cluster (if not already done)</li> </ul>"},{"location":"homelab/Dokploy/Plan/#post-installation-verification","title":"Post-Installation Verification","text":"<p>Check installation status: <pre><code># Verify Dokploy services are running\ndocker service ls\n\n# Check Dokploy logs\ndocker service logs dokploy_dokploy\n\n# Verify Traefik is accessible\ncurl -I http://localhost:8080\n</code></pre></p> <p>Expected services after installation: - <code>dokploy_dokploy</code> - Main Dokploy application - <code>dokploy_traefik</code> - Reverse proxy - <code>dokploy_postgres</code> - Database - <code>dokploy_redis</code> - Cache</p> <p>Installation Notes</p> <ul> <li>Firewall: Ensure port 3000 (Dokploy) and 8080 (Traefik) are accessible</li> <li>Resources: Dokploy requires at least 2GB RAM and 10GB disk space</li> <li>Backup: The PostgreSQL database contains all deployment configurations</li> <li>SSL: Consider configuring SSL certificates for production use</li> </ul>"},{"location":"homelab/Dokploy/Plan/#5-access-dokploy-web-ui","title":"5. Access Dokploy Web UI","text":"<p>Once Dokploy is installed, you can access the web interface to begin configuration and deployment management.</p>"},{"location":"homelab/Dokploy/Plan/#accessing-the-interface","title":"Accessing the Interface","text":"<p>Via Tailscale (recommended for security): <pre><code>http://metal0.tailnet-name.ts.net:3000\n</code></pre></p> <p>Via local network (if firewall allows): <pre><code>http://192.168.1.190:3000\n</code></pre></p> <p>Access Methods</p> <ul> <li>Tailscale: Most secure, works from anywhere</li> <li>Local network: Faster, but limited to LAN access</li> <li>Cloudflare Tunnel: For secure external access (advanced setup)</li> </ul>"},{"location":"homelab/Dokploy/Plan/#initial-setup-configuration","title":"Initial Setup Configuration","text":"<p>Complete the initial setup wizard:</p> <ol> <li>Create Admin User</li> <li>Set a strong password</li> <li>Use a valid email address for notifications</li> <li> <p>Enable two-factor authentication if available</p> </li> <li> <p>Configure Domain Settings</p> </li> <li>Set default domain suffix (e.g., <code>cooked.beer</code>)</li> <li>Configure DNS settings for automatic subdomain creation</li> <li> <p>Set up SSL certificate management</p> </li> <li> <p>Docker Registry Configuration</p> </li> <li>Configure Docker Hub credentials (if using private images)</li> <li>Set up private registry access (optional)</li> <li> <p>Configure image pull policies</p> </li> <li> <p>Cluster Settings</p> </li> <li>Verify Docker Swarm status</li> <li>Configure default networks</li> <li>Set resource limits and quotas</li> </ol> <p>Security Setup</p> <ul> <li>Strong passwords: Use a password manager for admin credentials</li> <li>HTTPS: Configure SSL certificates for production use</li> <li>Access control: Limit access to trusted networks or VPN</li> <li>Backup credentials: Store admin credentials securely</li> </ul>"},{"location":"homelab/Dokploy/Plan/#6-add-cluster-nodes","title":"6. Add Cluster Nodes","text":"<p>Expand the Docker Swarm cluster by adding worker nodes through the Dokploy interface.</p>"},{"location":"homelab/Dokploy/Plan/#adding-worker-nodes","title":"Adding Worker Nodes","text":"<p>From Dokploy UI: 1. Navigate to Cluster &gt; Add Node 2. Copy the Docker Swarm join command provided 3. The command will look similar to:    <pre><code>docker swarm join --token SWMTKN-1-xxxxx 192.168.1.190:2377\n</code></pre></p> <p>Execute on each worker node: <pre><code># SSH into each worker node\nssh metal1.tailnet-name.ts.net\n\n# Run the join command (replace with actual token)\ndocker swarm join --token SWMTKN-1-xxxxx 192.168.1.190:2377\n\n# Verify the node joined successfully\ndocker node ls\n</code></pre></p>"},{"location":"homelab/Dokploy/Plan/#adding-manager-nodes-optional","title":"Adding Manager Nodes (Optional)","text":"<p>For high availability, add additional manager nodes: <pre><code># Generate manager join token on existing manager\ndocker swarm join-token manager\n\n# Use the manager token on metal5\nssh metal5.tailnet-name.ts.net\ndocker swarm join --token SWMTKN-1-manager-token 192.168.1.190:2377\n</code></pre></p>"},{"location":"homelab/Dokploy/Plan/#cluster-verification","title":"Cluster Verification","text":"<p>Verify cluster status: <pre><code># Check all nodes from any manager\ndocker node ls\n\n# Verify services can be deployed across nodes\ndocker service create --replicas 3 --name test-service nginx\ndocker service ps test-service\n\n# Clean up test service\ndocker service rm test-service\n</code></pre></p> <p>Cluster Health Checks</p> <ul> <li>Node status: All nodes should show \"Ready\" status</li> <li>Manager quorum: Ensure odd number of managers (1, 3, or 5)</li> <li>Network connectivity: Verify overlay networks work between nodes</li> <li>Resource availability: Check CPU, memory, and disk space on all nodes</li> </ul>"},{"location":"homelab/Dokploy/Plan/#7-post-deployment-configuration","title":"7. Post-Deployment Configuration","text":""},{"location":"homelab/Dokploy/Plan/#essential-next-steps","title":"Essential Next Steps","text":"<p>Security Hardening: - Configure SSL certificates for Dokploy and Traefik - Set up proper firewall rules between nodes - Enable audit logging for Docker Swarm - Configure backup strategies for Dokploy database</p> <p>Monitoring Setup: - Deploy monitoring stack (Prometheus, Grafana) - Configure log aggregation (Loki, Fluentd) - Set up alerting for cluster health - Monitor resource usage across nodes</p> <p>Backup Strategy: - Backup Dokploy PostgreSQL database - Document cluster configuration - Create disaster recovery procedures - Test restore procedures regularly</p> <p>Production Readiness</p> <p>Before deploying production workloads:</p> <ul> <li>Test cluster failover scenarios</li> <li>Verify backup and restore procedures</li> <li>Configure monitoring and alerting</li> <li>Document operational procedures</li> <li>Train team members on cluster management</li> </ul>"},{"location":"homelab/Dokploy/Plan/#8-troubleshooting-common-issues","title":"8. Troubleshooting Common Issues","text":""},{"location":"homelab/Dokploy/Plan/#docker-swarm-issues","title":"Docker Swarm Issues","text":"<p>Node fails to join cluster: <pre><code># Check firewall ports (2377, 7946, 4789)\nsudo ufw status\n\n# Verify Docker daemon is running\nsudo systemctl status docker\n\n# Check network connectivity\nping 192.168.1.190\n\n# Reset and rejoin if necessary\ndocker swarm leave --force\ndocker swarm join --token &lt;new-token&gt; 192.168.1.190:2377\n</code></pre></p> <p>Service deployment failures: <pre><code># Check service status\ndocker service ps &lt;service-name&gt; --no-trunc\n\n# View service logs\ndocker service logs &lt;service-name&gt;\n\n# Check node constraints\ndocker node inspect &lt;node-name&gt;\n</code></pre></p>"},{"location":"homelab/Dokploy/Plan/#dokploy-issues","title":"Dokploy Issues","text":"<p>Web UI not accessible: <pre><code># Check Dokploy service status\ndocker service ls | grep dokploy\n\n# Verify port binding\nsudo netstat -tlnp | grep :3000\n\n# Check Dokploy logs\ndocker service logs dokploy_dokploy\n</code></pre></p> <p>Database connection issues: <pre><code># Check PostgreSQL service\ndocker service ps dokploy_postgres\n\n# Verify database connectivity\ndocker exec -it $(docker ps -q -f name=dokploy_postgres) psql -U postgres -d dokploy\n</code></pre></p>"},{"location":"homelab/Dokploy/Plan/#network-connectivity-issues","title":"Network Connectivity Issues","text":"<p>Tailscale connectivity problems: <pre><code># Check Tailscale status\nsudo tailscale status\n\n# Restart Tailscale service\nsudo systemctl restart tailscaled\n\n# Re-authenticate if necessary\nsudo tailscale up --ssh --accept-routes\n</code></pre></p> <p>Emergency Procedures</p> <p>If the cluster becomes unresponsive:</p> <ol> <li>Document the issue: Capture logs and error messages</li> <li>Check node health: Verify all nodes are accessible</li> <li>Restart services: Try restarting problematic services first</li> <li>Cluster recovery: Use <code>docker swarm init --force-new-cluster</code> as last resort</li> <li>Restore from backup: Have backup restoration procedures ready</li> </ol>"},{"location":"homelab/Dokploy/Plan/#9-maintenance-and-updates","title":"9. Maintenance and Updates","text":""},{"location":"homelab/Dokploy/Plan/#regular-maintenance-schedule","title":"Regular Maintenance Schedule","text":"<p>Weekly Tasks: - Check cluster health and node status - Review service logs for errors - Monitor resource usage - Verify backup completion</p> <p>Monthly Tasks: - Update Docker Engine on all nodes - Update Dokploy to latest version - Review and rotate secrets - Test disaster recovery procedures</p> <p>Quarterly Tasks: - Update Ubuntu Server on all nodes - Review security configurations - Capacity planning assessment - Documentation updates</p>"},{"location":"homelab/Dokploy/Plan/#update-procedures","title":"Update Procedures","text":"<p>Updating Docker Engine: <pre><code># Update on each node (one at a time)\nsudo apt update\nsudo apt upgrade docker-ce docker-ce-cli containerd.io\n\n# Restart Docker service\nsudo systemctl restart docker\n\n# Verify cluster health\ndocker node ls\n</code></pre></p> <p>Updating Dokploy: <pre><code># Check current version\ndocker service inspect dokploy_dokploy --format '{{.Spec.TaskTemplate.ContainerSpec.Image}}'\n\n# Update using Dokploy's update mechanism\ncurl -sSL https://dokploy.com/install.sh | sh\n</code></pre></p> <p>Update Best Practices</p> <ul> <li>Test updates: Always test in a development environment first</li> <li>Rolling updates: Update nodes one at a time to maintain availability</li> <li>Backup first: Create backups before major updates</li> <li>Monitor closely: Watch for issues during and after updates</li> <li>Rollback plan: Have a rollback strategy ready</li> </ul>"},{"location":"homelab/Dokploy/Plan/#10-resources-and-documentation","title":"10. Resources and Documentation","text":""},{"location":"homelab/Dokploy/Plan/#essential-documentation","title":"Essential Documentation","text":"<ul> <li>Docker Swarm Documentation: Official Docker Swarm guide</li> <li>Dokploy Documentation: Dokploy setup and configuration</li> <li>Tailscale Documentation: VPN setup and troubleshooting</li> <li>Ubuntu Server Guide: Operating system administration</li> </ul>"},{"location":"homelab/Dokploy/Plan/#useful-commands-reference","title":"Useful Commands Reference","text":"<p>Docker Swarm Management: <pre><code># Cluster status\ndocker node ls\ndocker service ls\ndocker stack ls\n\n# Service management\ndocker service create --name &lt;name&gt; &lt;image&gt;\ndocker service scale &lt;service&gt;=&lt;replicas&gt;\ndocker service update &lt;service&gt;\n\n# Troubleshooting\ndocker service ps &lt;service&gt; --no-trunc\ndocker service logs &lt;service&gt;\ndocker node inspect &lt;node&gt;\n</code></pre></p> <p>System Monitoring: <pre><code># Resource usage\nhtop\ndf -h\nfree -h\n\n# Network connectivity\nping &lt;host&gt;\nnetstat -tlnp\nss -tlnp\n</code></pre></p> <p>Congratulations!</p> <p>You now have a production-ready Docker Swarm cluster with Dokploy orchestration. This setup provides a solid foundation for deploying and managing containerised applications in your home lab environment.</p>"},{"location":"homelab/K8s/Homelab%20Night%201/","title":"Homelab Configuration Night 1: Development Environment Setup","text":"<p>This document chronicles the first night of setting up a Kubernetes-based home lab, focusing on preparing the development environment on Windows using WSL and Nix.</p> <p>Session Overview</p> <p>Goal: Set up a reproducible development environment for home lab deployment Duration: Evening session Key Technologies: WSL, Nix, Git, Terraform</p>"},{"location":"homelab/K8s/Homelab%20Night%201/#windows-desktop-setup-for-deployment","title":"Windows Desktop Setup for Deployment","text":"<p>Setting up Windows Subsystem for Linux (WSL) provides a native Linux environment for running deployment scripts and tools that aren't available in PowerShell.</p>"},{"location":"homelab/K8s/Homelab%20Night%201/#wsl-installation-and-configuration","title":"WSL Installation and Configuration","text":"<p>Install WSL (Windows Subsystem for Linux):</p> <pre><code># Enable WSL feature and install default distribution\nwsl --install\n</code></pre> <p>WSL Installation Notes</p> <ul> <li>Requires Windows 10 version 2004 or higher, or Windows 11</li> <li>May require a system restart after installation</li> <li>Enables both WSL and Virtual Machine Platform features</li> </ul> <p>Install Ubuntu 24.04 LTS: <pre><code># Install specific Ubuntu version for consistency\nwsl --install Ubuntu-24.04\n</code></pre></p> <p>Start Ubuntu 24.04 LTS: <pre><code># Launch the specific distribution\nwsl -d Ubuntu-24.04\n</code></pre></p> <p>First Launch Setup</p> <p>On first launch, Ubuntu will prompt you to:</p> <ul> <li>Create a new user account</li> <li>Set a password</li> <li>Update the system packages</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%201/#nix-package-manager-installation","title":"Nix Package Manager Installation","text":"<p>Install Determinate Nix for WSL: <pre><code># Install Nix package manager with Determinate Systems installer\ncurl -fsSL https://install.determinate.systems/nix | sh -s -- install --determinate\n</code></pre></p> <p>Why Nix?</p> <p>Nix provides:</p> <ul> <li>Reproducible environments: Exact same tools across different machines</li> <li>Declarative configuration: Infrastructure as code approach</li> <li>Isolation: No conflicts between different project dependencies</li> <li>Rollback capability: Easy to revert to previous configurations</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%201/#repository-setup-and-project-initialisation","title":"Repository Setup and Project Initialisation","text":"<p>Navigate to existing Git repository (if available): <pre><code># Access Windows filesystem from WSL\ncd /mnt/d/Git/homelab\n</code></pre></p> <p>Set up Linux home directory structure: <pre><code># Return to Linux home directory\ncd ~\n\n# Create organised directory structure\nmkdir -p git\ncd git\n</code></pre></p> <p>Clone the home lab repository: <pre><code># Clone the project repository\ngit clone https://github.com/RovxBot/homelab.git\n\n# Navigate to project directory\ncd homelab\n</code></pre></p> <p>Directory Structure Best Practices</p> <p>Windows Integration: - <code>/mnt/c/</code> - Access Windows C: drive - <code>/mnt/d/</code> - Access Windows D: drive - Keep Linux projects in <code>~/git/</code> for better performance</p> <p>Git Workflow: - Use SSH keys for authentication - Configure Git user settings in WSL - Consider using Git credential manager for Windows integration</p>"},{"location":"homelab/K8s/Homelab%20Night%201/#development-environment-activation","title":"Development Environment Activation","text":"<p>Enter Nix development shell: <pre><code># Activate the development environment with all required tools\nnix develop\n</code></pre></p> <p>Nix Development Shell</p> <p>The <code>nix develop</code> command creates an isolated environment with:</p> <ul> <li>Kubernetes tools (kubectl, helm, etc.)</li> <li>Terraform for infrastructure provisioning</li> <li>Ansible for configuration management</li> <li>Development utilities and dependencies</li> </ul> <p>Run initial configuration: <pre><code># Execute the configuration setup\nmake configure\n</code></pre></p>"},{"location":"homelab/K8s/Homelab%20Night%201/#configuration-parameters","title":"Configuration Parameters","text":"<p>The configuration process prompts for several key parameters that define the home lab environment:</p>"},{"location":"homelab/K8s/Homelab%20Night%201/#basic-settings","title":"Basic Settings","text":"<ul> <li>Text editor: <code>nano</code> (lightweight, beginner-friendly)</li> <li>Domain: <code>cooked.beer</code> (custom domain for services)</li> <li>Seed repository: <code>https://github.com/RovxBot/homelab</code></li> <li>Timezone: <code>Australia/Adelaide</code> (ACST/ACDT)</li> <li>Load balancer IP range: <code>10.0.0.0/24</code> (internal service IPs)</li> </ul> <p>Configuration Choices Explained</p> <ul> <li>Domain selection: Choose a domain you control for SSL certificates and DNS</li> <li>IP range: Use RFC 1918 private ranges that don't conflict with your LAN</li> <li>Timezone: Critical for log timestamps and scheduled tasks</li> <li>Editor: Can be changed later, nano is good for beginners</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%201/#server-infrastructure-configuration","title":"Server Infrastructure Configuration","text":"<p>Ansible inventory configuration: <pre><code>all:\n  vars:\n    control_plane_endpoint: 192.168.1.180  # Kubernetes API server endpoint\n    load_balancer_ip_pool:\n      - 10.0.0.0/24                        # MetalLB IP pool for services\nmetal:\n  children:\n    masters:                               # Kubernetes control plane nodes\n      hosts:\n        metal0:\n          ansible_host: 192.168.1.190\n          mac: 'D8:9E:F3:90:E8:31'\n          disk: nvme0n1\n          network_interface: enp0s31f6\n        metal1:\n          ansible_host: 192.168.1.191\n          mac: 'D8:9E:F3:10:E8:A8'\n          disk: nvme0n1\n          network_interface: enp0s31f6\n    workers:                               # Kubernetes worker nodes\n      hosts:\n        # Additional worker nodes to be added\n</code></pre></p> <p>Infrastructure Design</p> <p>Control Plane Setup: - High Availability: Multiple master nodes for redundancy - Load Balancer: MetalLB provides LoadBalancer services in bare metal - Network: Dell OptiPlex machines with consistent hardware</p> <p>Hardware Specifications: - Network Interface: <code>enp0s31f6</code> (Dell-specific naming) - Storage: NVMe SSDs (<code>nvme0n1</code>) for performance - MAC Addresses: Required for Wake-on-LAN functionality</p>"},{"location":"homelab/K8s/Homelab%20Night%201/#terraform-cloud-integration","title":"Terraform Cloud Integration","text":"<p>Terraform Workspace Configuration: - Workspace ID: <code>ws-yM6CtwT9NWtgTug3</code> - Purpose: Remote state management and collaborative infrastructure changes</p> <p>Terraform Workspace Setup</p> <p>The workspace ID configuration may need verification. Terraform Cloud workspaces provide:</p> <ul> <li>Remote state storage: Prevents state file conflicts</li> <li>Collaborative workflows: Team-based infrastructure management</li> <li>Variable management: Secure storage of sensitive configuration</li> <li>Plan/apply automation: CI/CD integration capabilities</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%201/#version-control-and-authentication","title":"Version Control and Authentication","text":"<p>Commit and push changes: <pre><code># Stage all configuration changes\ngit add .\n\n# Commit with descriptive message\ngit commit -m \"Initial home lab configuration - Night 1\"\n\n# Push to remote repository\ngit push origin main\n</code></pre></p> <p>GitHub Authentication: <pre><code># Configure Git with your details\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n# Use Personal Access Token for authentication\n# GitHub username/password authentication is deprecated\n</code></pre></p> <p>GitHub Token Setup</p> <p>Creating a Personal Access Token:</p> <ol> <li>Go to GitHub Settings &gt; Developer settings &gt; Personal access tokens</li> <li>Generate new token with appropriate scopes:</li> <li><code>repo</code> - Full repository access</li> <li><code>workflow</code> - GitHub Actions access (if using CI/CD)</li> <li>Store token securely (password manager recommended)</li> <li>Use token as password when prompted by Git</li> </ol>"},{"location":"homelab/K8s/Homelab%20Night%201/#session-summary-and-next-steps","title":"Session Summary and Next Steps","text":""},{"location":"homelab/K8s/Homelab%20Night%201/#what-was-accomplished-tonight","title":"What Was Accomplished Tonight","text":"<p>Development Environment Setup: - WSL installation and Ubuntu 24.04 LTS configuration - Nix package manager installation for reproducible environments - Project repository cloning and initial setup</p> <p>Configuration Baseline: - Basic home lab parameters defined - Ansible inventory structure created - Terraform workspace integration configured</p> <p>Version Control: - Git repository initialised with configuration - GitHub authentication configured with personal access tokens</p>"},{"location":"homelab/K8s/Homelab%20Night%201/#lessons-learned","title":"Lessons Learned","text":"<p>Key Insights</p> <ul> <li>WSL Integration: Provides excellent Linux tooling on Windows</li> <li>Nix Benefits: Reproducible environments eliminate \"works on my machine\" issues</li> <li>Configuration Management: Declarative approach makes infrastructure predictable</li> <li>Documentation: Real-time documentation captures decision rationale</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%201/#tomorrow-nights-agenda","title":"Tomorrow Night's Agenda","text":"<p>Hardware Preparation Tasks: 1. Physical Setup: Collect and prepare Dell OptiPlex 7050 mini PCs 2. BIOS Configuration:    - Enable Wake-on-LAN functionality    - Disable Secure Boot for Linux installation    - Configure boot priorities 3. Hardware Discovery:    - Collect actual MAC addresses from each unit    - Identify network interface names    - Verify storage device naming conventions 4. Power Infrastructure:    - Install additional power supplies when they arrive    - Plan rack/shelf organisation for mini PCs</p> <p>Technical Preparation: - Update Ansible inventory with real hardware details - Prepare Ubuntu Server installation media - Plan network addressing scheme - Document BIOS settings for consistency</p> <p>Hardware Preparation Tips</p> <ul> <li>Label everything: Physical labels help track which unit is which</li> <li>Document BIOS settings: Take photos of important configuration screens</li> <li>Test Wake-on-LAN: Verify functionality before final deployment</li> <li>Plan cable management: Consider network and power cable routing</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%201/#future-considerations","title":"Future Considerations","text":"<p>Expansion Planning: - Additional mini PCs pending power supply delivery - Scalability considerations for Kubernetes cluster growth - Storage expansion options (NAS integration) - Monitoring and observability stack planning</p> <p>Security Considerations: - Network segmentation planning - Certificate management strategy - Backup and disaster recovery procedures - Access control and authentication methods</p>"},{"location":"homelab/K8s/Homelab%20Night%202/","title":"Homelab Configuration Night 2: Hardware Setup and Discovery","text":"<p>This document chronicles the second night of home lab setup, focusing on physical hardware preparation, BIOS configuration, and system discovery for the Kubernetes cluster nodes.</p> <p>Session Overview</p> <p>Goal: Configure physical hardware and discover system specifications Hardware: Dell OptiPlex 7050 micro form factor desktops Duration: Evening session focused on hands-on hardware work</p>"},{"location":"homelab/K8s/Homelab%20Night%202/#hardware-foundation-dell-optiplex-7050-analysis","title":"Hardware Foundation: Dell OptiPlex 7050 Analysis","text":""},{"location":"homelab/K8s/Homelab%20Night%202/#why-dell-optiplex-7050","title":"Why Dell OptiPlex 7050?","text":"<p>Each unit is a Dell OptiPlex 7050 acquired from a local business upgrading their infrastructure. These micro form factor desktops represent an excellent balance for home lab use.</p> <p>Advantages of Micro Form Factor Desktops: - Compact footprint: Minimal rack/shelf space requirements - Quiet operation: Better cooling design than stacked laptops - Adequate performance: Sufficient CPU/RAM for Kubernetes workloads - Standardised hardware: Consistent specifications across units - Business-grade reliability: Enterprise components and build quality</p> <p>Future Hardware Availability</p> <p>The trend toward laptop-only business environments may impact the availability of quality micro desktops for home labbers. Consider acquiring units while they're still available from business upgrades.</p>"},{"location":"homelab/K8s/Homelab%20Night%202/#node-configuration-details","title":"Node Configuration Details","text":""},{"location":"homelab/K8s/Homelab%20Night%202/#metal0-primary-control-plane","title":"metal0 (Primary Control Plane)","text":"<ul> <li>IP Address: <code>192.168.1.190</code></li> <li>MAC Address: <code>D8:9E:F3:90:E8:31</code></li> <li>Secure Boot: Disabled (required for Linux installation)</li> <li>Wake on LAN: Enabled for LAN (remote power management)</li> <li>Network Interface: <code>enp0s31f6</code> (Dell-specific PCIe naming)</li> <li>Storage Device: <code>nvme0n1</code> (NVMe SSD primary storage)</li> <li>Role: Kubernetes control plane node</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%202/#metal1-secondary-control-plane","title":"metal1 (Secondary Control Plane)","text":"<ul> <li>IP Address: <code>192.168.1.191</code></li> <li>MAC Address: <code>D8:9E:F3:10:E8:A8</code></li> <li>Secure Boot: Disabled</li> <li>Wake on LAN: Enabled for LAN</li> <li>Network Interface: <code>enp0s31f6</code></li> <li>Storage Device: <code>nvme0n1</code></li> <li>Role: Kubernetes control plane node (HA)</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%202/#metal2-worker-node","title":"metal2 (Worker Node)","text":"<ul> <li>IP Address: <code>192.168.1.192</code></li> <li>MAC Address: <code>D8:9E:F3:90:DD-2B</code></li> <li>Secure Boot: Disabled</li> <li>Wake on LAN: Enabled for LAN</li> <li>Network Interface: <code>enp0s31f6</code></li> <li>Storage Device: <code>nvme0n1</code></li> <li>Role: Kubernetes worker node</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%202/#metal3-worker-node","title":"metal3 (Worker Node)","text":"<ul> <li>IP Address: <code>192.168.1.193</code></li> <li>MAC Address: <code>D8:9E:F3:90:E9:54</code></li> <li>Secure Boot: Disabled</li> <li>Wake on LAN: Enabled for LAN</li> <li>Network Interface: <code>enp0s31f6</code></li> <li>Storage Device: <code>nvme0n1</code></li> <li>Role: Kubernetes worker node</li> </ul> <p>Hardware Consistency</p> <p>Standardised Configuration: - All units share identical network interface naming (<code>enp0s31f6</code>) - Consistent NVMe storage device naming (<code>nvme0n1</code>) - Uniform BIOS settings across all nodes - Sequential IP addressing for easy management</p>"},{"location":"homelab/K8s/Homelab%20Night%202/#hardware-discovery-process","title":"Hardware Discovery Process","text":""},{"location":"homelab/K8s/Homelab%20Night%202/#the-challenge-device-naming-discovery","title":"The Challenge: Device Naming Discovery","text":"<p>The most challenging aspect was determining the correct network and storage device names. Dell's BIOS provides minimal information about Linux device naming conventions, requiring a hands-on discovery approach.</p> <p>Initial Approach (Unsuccessful): - Attempted to gather information during Ubuntu installation process - Installation screens didn't provide sufficient detail for automation needs</p>"},{"location":"homelab/K8s/Homelab%20Night%202/#successful-discovery-method","title":"Successful Discovery Method","text":"<p>Complete Ubuntu installation and system interrogation:</p> <p>Hardware Discovery Strategy</p> <p>When BIOS information is insufficient, a full OS installation provides the most reliable method for discovering Linux device names and system specifications.</p> <p>Storage Device Discovery: <pre><code># List all block devices with hierarchy\nlsblk\n\n# Output shows:\n# - Device names (nvme0n1, sda, etc.)\n# - Partition layout\n# - Mount points\n# - Device sizes and types\n</code></pre></p> <p>Network Interface Discovery: <pre><code># Display all network interfaces\nip link show\n\n# Alternative command for detailed information\nip addr show\n</code></pre></p> <p>Example output for metal0: - Network Interface: <code>enp0s31f6</code> - Naming Convention: PCIe-based predictable network interface names - Dell Specifics: Unique to Dell's hardware implementation</p> <p>Linux Network Interface Naming</p> <p>Modern Predictable Names: - <code>enp0s31f6</code> breaks down as:   - <code>en</code> = Ethernet   - <code>p0</code> = PCI bus 0   - <code>s31</code> = Slot 31   - <code>f6</code> = Function 6</p> <p>This naming ensures consistent interface names across reboots and hardware changes.</p>"},{"location":"homelab/K8s/Homelab%20Night%202/#storage-configuration-requirements","title":"Storage Configuration Requirements","text":""},{"location":"homelab/K8s/Homelab%20Night%202/#bios-storage-settings","title":"BIOS Storage Settings","text":"<p>Critical Discovery: RAID vs AHCI Configuration</p> <p>During the hardware setup process, it became apparent that additional BIOS configuration was required for proper Linux storage support.</p> <p>Required BIOS Changes: 1. Disable RAID Mode: Dell OptiPlex units default to RAID/Intel RST mode 2. Enable AHCI Mode: Required for proper NVMe and SATA device recognition 3. Verify Storage Detection: Ensure all storage devices are visible</p> <p>Storage Mode Impact</p> <p>RAID Mode Issues: - Linux may not detect NVMe devices properly - Requires additional drivers or kernel modules - Can cause installation failures</p> <p>AHCI Mode Benefits: - Native Linux support for all storage devices - Better performance for single-drive configurations - Simplified driver requirements</p> <p>BIOS Configuration Steps: 1. Enter BIOS setup (F2 during boot) 2. Navigate to Storage or Drive Configuration 3. Change from \"RAID On\" to \"AHCI\" 4. Save settings and exit 5. Verify storage devices are detected correctly</p> <p>BIOS Configuration Best Practices</p> <ul> <li>Document settings: Take photos of BIOS screens for reference</li> <li>Consistent configuration: Apply identical settings across all nodes</li> <li>Test before deployment: Verify storage detection after changes</li> <li>Backup settings: Some BIOS versions allow configuration export</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%202/#development-environment-docker-installation","title":"Development Environment: Docker Installation","text":""},{"location":"homelab/K8s/Homelab%20Night%202/#docker-setup-in-wsl-environment","title":"Docker Setup in WSL Environment","text":"<p>Returning to the development PC, Docker installation was required within the WSL environment to support container-based tooling and testing.</p>"},{"location":"homelab/K8s/Homelab%20Night%202/#docker-installation-process","title":"Docker Installation Process","text":"<p>Step 1: Configure Docker APT Repository <pre><code># Update package index and install prerequisites\nsudo apt-get update\nsudo apt-get install ca-certificates curl\n\n# Create keyrings directory for GPG keys\nsudo install -m 0755 -d /etc/apt/keyrings\n\n# Download and install Docker's official GPG key\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add Docker repository to APT sources\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Update package index with Docker repository\nsudo apt-get update\n</code></pre></p> <p>Step 2: Install Docker Components <pre><code># Install Docker Engine and related components\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre></p> <p>Docker Components Explained</p> <ul> <li>docker-ce: Docker Community Edition engine</li> <li>docker-ce-cli: Command-line interface tools</li> <li>containerd.io: Container runtime</li> <li>docker-buildx-plugin: Extended build capabilities</li> <li>docker-compose-plugin: Multi-container application management</li> </ul> <p>Step 3: User Permission Configuration <pre><code># Add current user to docker group (avoids sudo requirement)\nsudo usermod -aG docker $USER\n\n# Apply group membership changes\n# Option 1: Start new WSL session\n# Option 2: Use newgrp command\nnewgrp docker\n</code></pre></p> <p>Group Membership Security</p> <p>Adding users to the <code>docker</code> group grants root-equivalent access to the system. Only add trusted users to this group, as Docker containers can access the host filesystem and resources.</p>"},{"location":"homelab/K8s/Homelab%20Night%202/#development-environment-validation","title":"Development Environment Validation","text":"<p>Return to Nix Development Environment: <pre><code># Activate the development shell with all tools\nnix develop\n\n# Execute the build/deployment process\nmake\n</code></pre></p> <p>Development Environment Ready</p> <p>With Docker installed and user permissions configured, the development environment now supports:</p> <ul> <li>Container building: Local testing of Kubernetes manifests</li> <li>Development workflows: Full toolchain available in Nix shell</li> <li>Integration testing: Ability to test configurations locally</li> <li>CI/CD preparation: Foundation for automated deployment pipelines</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%202/#session-results-and-validation","title":"Session Results and Validation","text":""},{"location":"homelab/K8s/Homelab%20Night%202/#build-process-execution","title":"Build Process Execution","text":"<p>The culmination of Night 2's work was successfully executing the build process, validating that all components were properly configured and integrated.</p> <p>Build Output Analysis: </p> <p>Build Process Verification</p> <p>The successful <code>make</code> execution confirms:</p> <ul> <li>Nix environment: All development tools properly installed</li> <li>Docker integration: Container runtime accessible from development shell</li> <li>Configuration validity: Ansible inventory and Terraform configurations parsed correctly</li> <li>Dependency resolution: All required packages and tools available</li> </ul>"},{"location":"homelab/K8s/Homelab%20Night%202/#session-summary-and-achievements","title":"Session Summary and Achievements","text":""},{"location":"homelab/K8s/Homelab%20Night%202/#hardware-configuration-completed","title":"Hardware Configuration Completed","text":"<p>Physical Setup: - Four Dell OptiPlex 7050 units configured and documented - BIOS settings standardised across all nodes - Wake-on-LAN enabled for remote management - Storage configuration optimised for Linux</p> <p>System Discovery: - Network interface names identified (<code>enp0s31f6</code>) - Storage device names confirmed (<code>nvme0n1</code>) - MAC addresses collected for all units - IP address assignments documented</p> <p>Development Environment: - Docker installed and configured in WSL - User permissions properly set - Integration with Nix development shell verified - Build process successfully executed</p>"},{"location":"homelab/K8s/Homelab%20Night%202/#key-technical-insights","title":"Key Technical Insights","text":"<p>Hardware Standardisation Benefits: - Consistent device naming simplifies automation - Identical BIOS settings reduce configuration drift - Standardised hardware reduces troubleshooting complexity</p> <p>Discovery Process Lessons: - BIOS information insufficient for Linux device naming - Full OS installation required for accurate device discovery - Documentation during discovery process saves future time</p> <p>Development Environment Integration: - WSL provides excellent Linux tooling on Windows - Docker integration enables local testing capabilities - Nix ensures reproducible development environments</p>"},{"location":"homelab/K8s/Homelab%20Night%202/#next-steps-and-future-sessions","title":"Next Steps and Future Sessions","text":"<p>Immediate Next Actions: 1. OS Installation: Deploy Ubuntu Server on all nodes 2. Network Configuration: Configure static IP addresses 3. SSH Setup: Enable key-based authentication 4. Ansible Preparation: Update inventory with discovered hardware details</p> <p>Future Session Planning: - Night 3: Ubuntu Server installation and basic configuration - Night 4: Kubernetes cluster initialisation - Night 5: Application deployment and testing</p> <p>Progress Assessment</p> <p>Night 2 successfully completed the hardware foundation phase. All physical nodes are configured and ready for OS installation, with development environment fully prepared for the next phase of deployment.</p>"}]}